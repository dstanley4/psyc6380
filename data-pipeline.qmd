# Data Science Pipelines in R

| Associated Files                         |
|------------------------------------------|
| [data-qualtrics.csv](data-qualtrics.csv) |
| [my-project.zip](my-project.zip) |

## Introduction

In the previous chapter, you learned how to read Qualtrics data and prepare it for analysis using a single R script. That approach works well for learning---but as your projects grow more complex, a single script becomes unwieldy. In this tutorial, you'll learn how to organize your work into a **data science pipeline**: a series of modular scripts that each handle one specific task.

```{r}
#| echo: false

knitr::include_graphics("ch_workflow/images/pipeline.png")
```

Recall we first saw the pipeline in this figure in a previous chapter. Today we will create a series of scripts. Most of these scripts correspond to the "Processing Code" step in the above figure. That is, we have expanded the Processing Code to have its own multi-step pipeline. In this chapter, we walk you through creating that pipeline.

Note that that data set we use in this chapter is not the same as the one in the previous chapter. The data set for this chapter has been modified to include missing data to illustrate how to evaluate and handle missing data in a pipeline. 

## Why Use a Pipeline?

Consider the single-script approach you've been using. It might look something like this:

```{r}
#| eval: false

# Everything in one file...
library(tidyverse)
library(janitor)

# Import
raw_data <- read_csv("data-qualtrics.csv", skip = 3)

# Clean
analytic_data <- raw_data |>
  select(-StartDate, -EndDate, ...) |>
  clean_names()

# Recode Likert scales
# ... 50 more lines ...

# Create scales
# ... 30 more lines ...

# Exclusions
# ... 20 more lines ...

# Analysis
# ... and so on
```

This approach has several problems:

1.  **Difficult to debug**: When something goes wrong on line 247, finding the cause is tedious.
2.  **Hard to modify**: Changing one part risks breaking another.
3.  **Poor collaboration**: Lab members can't work on different sections simultaneously.
4.  **No checkpoints**: If you make a mistake, you must re-run everything from scratch.

A pipeline solves these problems by breaking your workflow into discrete, manageable steps.

## The Pipeline Structure

Our data science pipeline uses a specific file and folder structure to keep everything organized. Here's an overview of the structure. We will recreate this structure in this chapter. You will create each file and folder as we go. Then you will paste the code for each step into the appropriate script file. Alternatively, you can download the completed project from the associated files at the top of this page.

```         
My Project/
├── 00-script-master.R      # Runs the entire pipeline     (Preprocessing Code)
├── 01-import.R             # Load and initial setup       (Preprocessing Code)
├── 02-clean-recode.R       # Factors and Likert recoding  (Preprocessing Code)
├── 03-missing-data.R       # Evaluate missingness         (Preprocessing Code)
├── 04-create-scales.R      # Compute scale scores         (Preprocessing Code)
├── 05-exclusions.R         # Apply exclusion criteria     (Preprocessing Code)
├── 06-analysis-wrapper.R   # Analysis and visualization   (Analytic Code)
├── 07-analysis.R           # Analysis and visualization   (Analytic Code)
├── data-raw/                # Folder: Original data (NEVER modify)
├── data-interim/            # Folder: Checkpoint files between steps
├── data-processed/          # Folder: Final analytic dataset
└── output/                  # Folder: Analysis output, tables, figures, etc.
```

This structure follows the principle of separating **raw data**, **interim data**, **processed data**, and **output**. Each script reads from the previous step and writes to the next.

## Output naming conventions

We use clear, consistent naming conventions for files we create. For examples figures will begin with `figure-`, tables with `table-`, and data files with `data-`. This makes it easy to identify file types at a glance.

## Create Required Folders

Begin by creating the necessary folders in your project directory. You can do this manually or with R code. But we will begin mannually for clarity.

First, create a folder with the name of your project. This folder might be called "My-Thesis" or something similar. Today we will call this folder "My Project".

Second, create empty R script files inside "My Project" folder named as follows:

-   `00-script-master.R`

-   `01-import.R`

-   `02-clean-recode.R`

-   `03-missing-data.R`

-   `04-create-scales.R`

-   `05-exclusions.R`

-   `06-analysis-wrapper.R`

-   `07-analysis.R`

Third, inside "My Project", create the following folders:

-   `data-raw/`

-   `data-interim/`

-   `data-processed/`

-   `output/`

Next, place your raw data file (e.g., `data-qualtrics.csv`) inside the `data-raw/` folder. This file should never be modified.



## The Master Script

The master script is the conductor of your pipeline. It loads all necessary packages once, then calls each step in order. 

```{r}
#| eval: false
#| code-fold: false

# 00-script-master.R

# Date: 2026-01-15 (Use YYYY-MM-DD format)
# Name: [Your Name]
# Purpose: Master - Runs all data preparation steps in order

# 1. Setup Environment ----------------------------------------------------
library(tidyverse)
library(janitor)
library(skimr)
library(naniar) # Package for missing data visualization

# Clear memory to ensure reproducibility
rm(list = ls()) 

# Create directories if they don't exist (helpful for students)
if(!dir.exists("data-interim")) dir.create("data-interim")
if(!dir.exists("data-processed")) dir.create("data-processed")
if(!dir.exists("output")) dir.create("output")


# 2. Execute Pipeline -----------------------------------------------------

# Step 1: Import and Anonymize
message("--- Running Step 1: Import ---")
source("01-import.R", echo = TRUE)

# Step 2: Cleaning and Recoding
message("--- Running Step 2: Cleaning ---")
source("02-clean-recode.R", echo = TRUE)

# Step 3: Evaluate missing data
message("--- Running Step 3: Evaluate missing data ---")
source("03-missing-data.R", echo = TRUE)

# Step 4: Create scales
message("--- Running Step 4: Create scales ---")
source("04-create-scales.R", echo = TRUE)

# Step 5: Exclusions
message("--- Running Step 5: Excluding participants ---")
source("05-Exclusions.R", echo = TRUE)

# Step 6: Analysis Wraper
message("--- Running Step 6: Analysis and Visualization ---")
source("06-analysis-wrapper.R", echo = TRUE)


message("--- Pipeline Complete! ---")


```

Notice several important features:

-   `source()` with `echo = TRUE` shows you what each script is doing
-   `message()` calls provide clear progress indicators
-   All packages are loaded once at the beginning

## Step 1: Import

The import script handles loading raw data and initial cleanup. This is essentially the first part of your single script, isolated:

```{r}
#| eval: false
#| code-fold: false
 

# 01-import.R

# Step 1: Import and Initial Setup

# Load Data ---------------------------------------------------------------
survey_file <- "data-raw/data-qualtrics.csv"

# Read header row for names
col_names <- names(read_csv(survey_file, n_max = 0, show_col_types = FALSE))

# Read data (skipping Qualtrics header rows 2 and 3)
raw_data <- read_csv(survey_file, 
                     col_names = col_names, 
                     skip = 3, 
                     show_col_types = FALSE,
                     na = c("", "NA","999")) # Treat blank cells as NA
                    

# Select Columns ----------------------------------------------------------
# NOTE: We comment out "Duration" so we can use it for exclusions later!
cols_to_remove <- c("StartDate", "EndDate", "Status", "IPAddress", "Progress",
                    "Finished", "RecordedDate", "ResponseId", "RecipientLastName",
                    "RecipientFirstName", "RecipientEmail", "ExternalReference",
                    "LocationLatitude", "LocationLongitude", "DistributionChannel",
                    "UserLanguage")

analytic_data_survey <- raw_data |> 
  select(!any_of(cols_to_remove)) |> 
  clean_names() |>        
  remove_empty("rows") |> 
  remove_empty("cols")

# Create Participant ID ---------------------------------------------------
# Best practice: Create ID immediately so we can track rows if order changes
analytic_data_survey <- analytic_data_survey %>%
  mutate(participant_id = row_number()) %>% 
  relocate(participant_id) # Move to first column

# Save Interim File -------------------------------------------------------
# We use .rds because it preserves R formatting (unlike CSV)
write_rds(analytic_data_survey, "data-interim/01-imported.rds")

```

**Key insight**: We save using `.rds` format, not `.csv`. The RDS format preserves R data types (factors, dates, etc.) exactly as they are. CSV files lose this information.

## Step 2: Clean and Recode

This script handles factor creation and Likert scale recoding:

```{r}
#| eval: false
#| code-fold: false


# 02-clean-recode.R

# Step 2: Cleaning, Factors, and Recoding

# Load Previous Step ------------------------------------------------------
analytic_data_survey <- read_rds("data-interim/01-imported.rds")

# Factor Handling ---------------------------------------------------------
# Convert sex to factor and fix levels
analytic_data_survey <- analytic_data_survey %>%
  mutate(sex = as_factor(sex)) %>% 
  mutate(sex = fct_relevel(sex, "female", "intersex", "male"))

# Safety Check: Warn if new genders appear
expected_sex <- c("female", "intersex", "male")
unexpected <- setdiff(levels(analytic_data_survey$sex), expected_sex)
if (length(unexpected) > 0) {
  warning("Check Data! Unexpected sex levels found: ", paste(unexpected, collapse = ", "))
}

# Likert Recoding (Text to Numbers) ---------------------------------------

# Define Mappings
likert7_recode <- c(
  "Strongly Disagree" = 1,
  "Moderately Disagree" = 2,
  "Slightly Disagree" = 3,
  "Neither Agree nor Disagree" = 4,
  "Slightly Agree" = 5,
  "Moderately Agree" = 6,
  "Strongly Agree" = 7
)

likert5_recode <- c(
  "Strongly Disagree" = 1,
  "Disagree" = 2,
  "Neutral" = 3, 
  "Agree" = 4,
  "Strongly Agree" = 5
)

# Apply Mappings
analytic_data_survey <- analytic_data_survey %>%
  # Recode 7-point scales
  mutate(across(
    .cols = contains("likert7"), 
    .fns = ~ likert7_recode[.x]
  )) %>%
  # Recode 5-point scales
  mutate(across(
    .cols = contains("likert5"), 
    .fns = ~ likert5_recode[.x]
  ))

# Reverse Keying ----------------------------------------------------------
analytic_data_survey <- analytic_data_survey %>%
  mutate(across(
    .cols = ends_with("_likert7rev"),
    .fns = ~ (7 + 1) - .x
  )) %>%
  # Rename columns to remove the 'rev' suffix now that they are fixed
  rename_with(
    .fn = ~ str_replace(.x, "_likert7rev", "_likert7"),
    .cols = ends_with("_likert7rev")
  )

# Save Interim File -------------------------------------------------------
write_rds(analytic_data_survey, "data-interim/02-cleaned.rds")

```

Notice how our naming conventions (`likert7`, `likert7rev`, `likert5`) make it possible to apply recoding to multiple columns at once using `contains()` and `ends_with()`. This is why consistent naming matters.

## Step 3: Missing Data Evaluation

Before creating scales, we should understand our missing data. This step generates reports but doesn't modify the data. Importantly, we need to check the reports it creates in the `output` directory after we run this code. The amount and location of the missing data may suggest a different course of action than the default approach used in these scripts.

```{r}
#| eval: false
#| code-fold: false

# 03-missing-data.R

# Step 3: Missing Data Evaluation 

# Load Previous Step ------------------------------------------------------
analytic_data_survey <- read_rds("data-interim/02-cleaned.rds")


# Define Scale Items ------------------------------------------------------
# We select the columns relevant for scoring to check them specifically
scale_items <- analytic_data_survey %>%
  select(starts_with("aff_com"),
         starts_with("contin_com"),
         starts_with("norm_com"),
         starts_with("job_aff"))

# Missing Data Diagnosis --------------------------------------------------

# 1. Text Report: Items with the most missing data
message("--- Missing Data Report by Item ---")
missing_summary <- scale_items %>%
  miss_var_summary() %>%  # From naniar
  filter(n_miss > 0) 

print(missing_summary)
write_csv(missing_summary, "output/data-table-missing-by-item.csv")

# 2. Text Report: Participants with too much missing data
# Check if any participant is missing more than 20% of the scale items
message("--- Participants with > 20% Missing Data ---")
high_missing_participants <- analytic_data_survey %>%
  rowwise() %>%
  mutate(pct_missing = mean(is.na(c_across(c(starts_with("aff_com"),
                                             starts_with("contin_com"),
                                             starts_with("norm_com"),
                                             starts_with("job_aff"))))) * 100) %>%
  ungroup() %>%
  filter(pct_missing > 20) %>%
  select(participant_id, pct_missing)

print(high_missing_participants)
write_csv(high_missing_participants, "output/data-table-missing-by-participant.csv")

# 3. Visual Report
# Generates a map of missingness (Black = Missing, Grey = Present)
# We use 'print()' to ensure the plot renders when running from a script
message("--- Generating Missing Data Map ---")
plot_missing <- vis_miss(scale_items) +
  labs(title = "Missing Data Map (Scale Items Only)") +
  theme(axis.text.x = element_text(angle = 60, hjust = 0, vjust = 0, size = 16),
        axis.text.x.top = element_text(margin = margin(b = 2)),
        axis.text.y = element_text(size = 16),
        plot.title = element_text(size = 16),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 16),
        plot.margin = margin(t = 10, r = 10, b = 10, l = 10))

ggsave("output/figure-missing-data-map.png", plot = plot_missing, width = 20, height = 20, dpi = "print")

# Data not altered so no need to save

```

This step saves diagnostic outputs to the `output/` folder. You can review these to decide whether any participants should be excluded for excessive missing data.

## Step 4: Create Scales

Now we compute scale scores by averaging items:

```{r}
#| eval: false
#| code-fold: false

# 04-create-scales.R

# Step 4: Scale Creation 

# Load Previous Step ------------------------------------------------------
analytic_data_survey <- read_rds("data-interim/02-cleaned.rds")


# Create Scale Scores -----------------------------------------------------
# Note: We use rowwise() for accurate mean calculations across columns
analytic_data_survey <- analytic_data_survey %>% 
  rowwise() %>% 
  mutate(
    affective_commitment = mean(c_across(starts_with("aff_com")), na.rm = TRUE),
    continuance_commitment = mean(c_across(starts_with("contin_com")), na.rm = TRUE),
    normative_commitment = mean(c_across(starts_with("norm_com")), na.rm = TRUE),
    job_satisfaction = mean(c_across(starts_with("job_aff")), na.rm = TRUE)
  ) %>%
  ungroup() # Always ungroup after rowwise()!

# Clean up Item columns (Optional - keeps dataset clean)
analytic_data_survey <- analytic_data_survey %>%
  select(-starts_with("aff_com"),
         -starts_with("contin_com"),
         -starts_with("norm_com"),
         -starts_with("job_aff")) 

# Save Interim File -------------------------------------------------------
write_rds(analytic_data_survey, "data-interim/03-scales-created.rds")

```

**Important**: Always call `ungroup()` after `rowwise()`. Forgetting this can cause unexpected behavior in subsequent operations.

## Step 5: Exclusions

This step applies your preregistered exclusion criteria:

```{r}
#| eval: false
#| code-fold: false

# 05-exclusions.R

# Step 5: Exclusions

# Rules for excluding participants should be preregistered.
# In this example, we exclude participants who completed the survey in under 2 minutes.


# Load Previous Step ------------------------------------------------------
analytic_data_survey <- read_rds("data-interim/03-scales-created.rds")

# Exclusions --------------------------------------------------------------
# Filter out speeders (Requires 'duration_in_seconds' from Step 1)
initial_n <- nrow(analytic_data_survey)

# Only keep participants with duration >= 120 seconds (2 minutes)
analytic_data_survey <- analytic_data_survey %>%
  filter(duration_in_seconds >= 120)

# Print a message telling the student how many were dropped
final_n <- nrow(analytic_data_survey)
message(paste("Dropped", initial_n - final_n, "participants due to speed checks."))

# Final Save --------------------------------------------------------------
write_rds(analytic_data_survey, "data-processed/analytic-data-final.rds")

# Display final structure
glimpse(analytic_data_survey)


```

Note that this script saves to `data-processed/`, not `data-interim/`. This signals that the data is now ready for analysis.

## Step 6: Analysis Wrapper

This script runs the analysis script but captures the output.

```{r}
#| eval: false
#| code-fold: false

# 06-analysis-wrapper.R

# Step 6: Analysis Wraper

# This wrapper runs the analysis script and captures its output to a text file
# The text file is saved in the output directory
# The analysis script is assumed to be "07-analysis.R"

capture.output(source("07-analysis.R", echo =  TRUE),
                      file = "output/analysis-output.txt")


```

## Step 7: Analysis

Finally, the analysis script loads the clean data and performs your statistical analyses. Note that this file is not directly run in the 00-script-master.R; instead, it is called via the wrapper in Step 6. We do this so the output can be captered to a text file and placed in the output folder.

```{r}
#| eval: false
#| code-fold: false

# 07-analysis.R

# Step 7: Analysis

# Rules for excluding participants should be preregistered.
# In this example, we exclude participants who completed the survey in under 2 minutes.


# Clear memory to ensure reproducibility
rm(list = ls()) 

# Set random seed for reproducibility (e.g., geom_jitter)
set.seed(12345)

# 1. Setup Environment for Analyses ----------------------------------------------------
library(tidyverse)
library(GGally)
library(skimr)
library(apaTables)

# Load Previous Step ------------------------------------------------------
analytic_data_survey <- read_rds("data-processed/analytic-data-final.rds")

# Analysis ----------------------------------------------------------------


skim(analytic_data_survey)



# Example Analysis: Descriptive Statistics

data_plot <- analytic_data_survey |>
  select(contains("commitment")) 
  

apa.cor.table(data_plot,
                  filename = "output/table-correlation-descriptive-statistics.doc",
                  table.number = 1)


font_size <- 10
cor_plot <- ggpairs(data_plot,
                    upper = "blank",
                    lower = list(continuous = wrap("points", alpha = 0.3)),
                    diag = list(continuous = wrap("barDiag",
                                                  bins = 15,
                                                  color = "black",
                                                  fill = "black",
                                                  alpha = 1))) +
  theme_linedraw(font_size) +
  theme(panel.spacing = unit(.8, "lines"))

ggsave("output/figure-pairs-plot.png",
        plot = cor_plot,
        width = 7,
        height = 7,
        units = "in",
        dpi = 300)

print(cor_plot)


```

## Converting Your Single Script

To convert your existing single script to a pipeline:

1.  **Create the folder structure** shown above
2.  **Identify natural breakpoints** in your code (import, cleaning, scales, exclusions, analysis)
3.  **Move each section** to its own numbered script
4.  **Add `read_rds()` at the start** of each script (except step 1)
5.  **Add `write_rds()` at the end** of each script
6.  **Create a master script** that sources each step in order

## Benefits of the Pipeline Approach

The pipeline approach offers several advantages over a single script:

| Single Script                | Pipeline Approach                        |
|------------------------------|------------------------------------------|
| Everything in one file       | Modular, numbered scripts                |
| Difficult to debug           | Each step is isolated and testable       |
| Must re-run everything       | Can re-run from any checkpoint           |
| Hard to collaborate          | Team members can work on different steps |
| No clear progress indicators | Messages show which step is running      |

The pipeline approach requires a bit more setup, but the benefits for debugging, collaboration, and reproducibility make it well worth the effort---especially as your projects grow in complexity.

::: callout-tip
## Using AI is Easier with a Pipeline Approach

The modular pipeline structure makes it easier to work with AI assistants like ChatGPT or Claude. You can share individual scripts for help without overwhelming the AI with your entire project.
:::

## Output from the Pipeline

To run the entire pipeline, simply open `script-master.R` and run it. You can also run individual scripts if you need to re-do just one step (for example, if you change your exclusion criteria, you only need to re-run steps 5 and 6).

Let's run the master script now to see the pipeline in action!

Recall, running the script places the missing data evaluation in the output folder. Let's check that folder to see the results of that step.

```{r}
#| include: false
#| echo: false
library(here)
library(tidyverse)
```

### Missing data by item

We can see the percentage of missing data for each item by examining the CSV file created in the output folder called `data-table-missing-by-item.csv`.

```{r}
#| echo: false
missing_data_by_item <- read_csv(here("ch_data_pipeline/", "data-table-missing-by-item.csv"), show_col_types = FALSE)

knitr::kable(missing_data_by_item)
```

### Missing data by person

We can see the percentage of missing data for each person by examining the CSV file created in the output folder called `data-table-missing-by-participant.csv`.

```{r}
#| echo: false
missing_data_by_person <- read_csv(here("ch_data_pipeline/", "data-table-missing-by-participant.csv"), show_col_types = FALSE)

knitr::kable(missing_data_by_person)
```

### Visual missing data map

We can see the visual missing data map created in the output folder called `figure-missing-data-map.png`. This visual map helps identify patterns in the missing data across items and participants. The shape of the rectange corresponds to the number of participants (rows) and items (columns). The black areas indicate where I deleted cells in the spreadsheet to artifially create missing data for this example.

```{r}
#| echo: false
#| output-width: "70%"
knitr::include_graphics(here("ch_data_pipeline/", "figure-missing-data-map.png"))
```

### Pairs plot

We can see the pairs plot created in the output folder called `figure-pairs-plot.png`. The ggpairs function lets us see relationships among all the scales at once - and see the histogram for each scale along the diagonal.

```{r}
#| echo: false
#| output-width: "70%"
knitr::include_graphics(here("ch_data_pipeline/", "figure-pairs-plot.png"))
```

## Using SPSS Data?

If your data is in SPSS format (.sav), you can easily read it into R using the `haven` package. Here's how to modify the import script to handle SPSS files. Simply replace the read_csv line in `01-import.R` with the following code:

```{r}
#| eval: false
library(haven) 
# Read SPSS data
raw_data <- read_sav("data-raw/data-qualtrics.sav")

# Assign value labels (e.g., 1 in SPSS sex column becomes "male" if that's how you set up the value labels in SPSS)
raw_data <- as_factor(raw_data)

# check which columns are now factors after using as_factor()
glimpse(raw_data)

```
