[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSYC 6380 Psychological Applications of Multivariate Analysis",
    "section": "",
    "text": "Preface\nThis is a Quarto book designed to make it easier use R code with the textbook examples. In some cases I provide extra R code to make life easier for you (e.g., obtaining data from GitHub). In other cases, I use different code from the book to maintain the code-style we use in the course (i.e., a tidyverse approach to R)."
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Chapter 1",
    "section": "",
    "text": "No R-code in this chapter."
  },
  {
    "objectID": "chapter2.html#page-12",
    "href": "chapter2.html#page-12",
    "title": "2  Chapter 2",
    "section": "2.1 Page 12",
    "text": "2.1 Page 12\n\n2.1.1 Normal Distribution\n\nlibrary(tidyverse)\n\npopulation_data <- data.frame(weights = rnorm(1000000, 80, 10))\n\nweight_graph <- ggplot(data = population_data,\n                   mapping = aes(x = weights)) +\n  geom_density() +\n  scale_x_continuous(name = \"Distribution of Weights\") +\n  scale_y_continuous(name = \"Density\") +\n  theme_classic()\n\nprint(weight_graph)\n\n\n\n\n\n\n2.1.2 Skewed Distribution\n\nlibrary(tidyverse)\n\nincome_data <- data.frame(income = rf(1000000, df1 = 5, df2 = 2000))\n\nincome_graph <- ggplot(data = income_data,\n                   mapping = aes(x = income)) +\n  geom_density() +\n  scale_x_continuous(name = \"Distribution of Income\", \n                     breaks = seq(0,  6, by = 2)) +\n  scale_y_continuous(name = \"Density\",\n                     breaks = seq(0, .8, by = .2)) +\n\n  theme_classic()\n\nprint(income_graph)"
  },
  {
    "objectID": "chapter2.html#page-27",
    "href": "chapter2.html#page-27",
    "title": "2  Chapter 2",
    "section": "2.2 Page 27",
    "text": "2.2 Page 27\n\n2.2.1 Standardized Scores\n\nsample1_oz <- c(40, 45, 50, 55, 60, 65, 70)\nz_sample1_oz <- scale(sample1_oz, center = TRUE, scale = TRUE)\n\n\nsample2_grams <- c(1100, 1150, 1200, 1400, 1700, 1725, 1775)\nz_sample2_grams <- scale(sample2_grams, center = TRUE, scale = TRUE)\n\n\n\n\n\n\nsample1_oz\nsample2_grams\nz_sample1_oz\nz_sample2_grams\n\n\n\n\n40\n1100\n-1.3887301\n-1.1405606\n\n\n45\n1150\n-0.9258201\n-0.9706899\n\n\n50\n1200\n-0.4629100\n-0.8008191\n\n\n55\n1400\n0.0000000\n-0.1213362\n\n\n60\n1700\n0.4629100\n0.8978881\n\n\n65\n1725\n0.9258201\n0.9828235\n\n\n70\n1775\n1.3887301\n1.1526942"
  },
  {
    "objectID": "chapter2.html#page-30",
    "href": "chapter2.html#page-30",
    "title": "2  Chapter 2",
    "section": "2.3 Page 30",
    "text": "2.3 Page 30\n\n2.3.1 Correlation and Covariance\n\nsample1_oz <- c(40, 45, 50, 55, 60, 65, 70)\nsample1_length = c(31, 33, 37, 38, 42, 45, 48)\n\ncov_sample1 <- cov(sample1_oz, sample1_length)\nprint(cov_sample1)\n\n[1] 66.66667\n\ncorrelation_sample1 <- cor(sample1_oz, sample1_length)\nprint(correlation_sample1)\n\n[1] 0.9950372\n\n\nBut also note:\n\n# covariance is like correlation, but with the standard deviations included\n\ncorrelation_sample1 * sd(sample1_oz) * sd(sample1_length)\n\n[1] 66.66667\n\n# You get the same value as the covariance\nprint(cov_sample1)\n\n[1] 66.66667"
  },
  {
    "objectID": "chapter2.html#page-34",
    "href": "chapter2.html#page-34",
    "title": "2  Chapter 2",
    "section": "2.4 Page 34",
    "text": "2.4 Page 34\n\n2.4.1 Nations 2018\n\n2.4.1.1 Activate packages\n\nlibrary(usethis) # use_github_file() \nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \nlibrary(skimr) # skim()\n\n\n\n2.4.1.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/Nations2018.csv\",\n                save_as = \"nations2018.csv\")\n\n\n\n2.4.1.3 Load data from your computer\n\nnations2018 <- read_csv(\"nations2018.csv\") %>% \n  clean_names()\n\n\n\n2.4.1.4 Inspect data\n\nnations2018 %>% \n  glimpse()  \n\nRows: 8\nColumns: 4\n$ nation   <chr> \"Canada\", \"Finland\", \"France\", \"Germany\", \"Italy\", \"Japan\", \"…\n$ expend   <dbl> 21.0, 22.7, 23.4, 19.9, 19.0, 19.7, 18.5, 14.1\n$ econopen <dbl> 64.5, 76.2, 62.7, 87.4, 59.1, 34.6, 60.8, 27.1\n$ perlabor <dbl> 25.9, 60.3, 8.8, 16.5, 34.4, 17.0, 23.4, 10.1\n\n\nOr use\n\nnations2018 %>% \n  view()  \n\n\n\n\n\n\nnation\nexpend\neconopen\nperlabor\n\n\n\n\nCanada\n21.0\n64.5\n25.9\n\n\nFinland\n22.7\n76.2\n60.3\n\n\nFrance\n23.4\n62.7\n8.8\n\n\nGermany\n19.9\n87.4\n16.5\n\n\nItaly\n19.0\n59.1\n34.4\n\n\nJapan\n19.7\n34.6\n17.0\n\n\nUnited Kingdom\n18.5\n60.8\n23.4\n\n\nUnited States\n14.1\n27.1\n10.1\n\n\n\n\n\n\n\n2.4.1.5 Descriptive with skim()\n\nlibrary(skimr)\n\nnations2018 %>% \n  skim()  \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n8\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nnation\n0\n1\n5\n14\n0\n8\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nexpend\n0\n1\n19.79\n2.87\n14.1\n18.88\n19.80\n21.42\n23.4\n▂▁▅▇▅\n\n\neconopen\n0\n1\n59.05\n19.87\n27.1\n52.98\n61.75\n67.42\n87.4\n▅▁▇▂▅\n\n\nperlabor\n0\n1\n24.55\n16.72\n8.8\n14.90\n20.20\n28.02\n60.3\n▇▃▂▁▂\n\n\n\n\n\n\n\n2.4.1.6 Descriptives with describe()\nAlternatively, you could use the describe() command from the psych package as per the book. But, NEVER use library(psych) it will break the tidyverse. Instead use psyc:: before each psych package command.\n\n# psych package must be installed. But do not use library(psych)\n# Notice how psych creates a mean for the nation column - which makes no sense\nnations2018 %>% \n  psych::describe()  \n\n         vars n  mean    sd median trimmed   mad  min  max range  skew kurtosis\nnation*     1 8  4.50  2.45   4.50    4.50  2.97  1.0  8.0   7.0  0.00    -1.65\nexpend      2 8 19.79  2.87  19.80   19.79  1.85 14.1 23.4   9.3 -0.60    -0.62\neconopen    3 8 59.05 19.87  61.75   59.05 12.75 27.1 87.4  60.3 -0.31    -1.29\nperlabor    4 8 24.55 16.72  20.20   24.55 11.71  8.8 60.3  51.5  1.04    -0.19\n           se\nnation*  0.87\nexpend   1.01\neconopen 7.02\nperlabor 5.91\n\n\n\n\n\n2.4.2 GSS 2018\n\nlibrary(usethis) # use_github_file \nlibrary(tidyverse) # read_csv \nlibrary(janitor) # clean_names() \nlibrary(skimr) # skim\n\n\n2.4.2.1 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/GSS2018.csv\",\n                save_as = \"gss2018.csv\")\n\n\n\n2.4.2.2 Load data from your computer\n\ngss2018 <- read_csv(\"gss2018.csv\") %>% \n  clean_names()\n\n\n\n2.4.2.3 Inspect data\n\ngss2018 %>% \n  glimpse()  \n\nRows: 2,315\nColumns: 28\n$ id         <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ female     <chr> \"male\", \"female\", \"male\", \"female\", \"male\", \"female\", \"fema…\n$ age        <dbl> 43, 74, 42, 63, 71, 67, 59, 43, 62, 55, 59, 34, 61, 44, 41,…\n$ cohort     <dbl> 1975, 1944, 1976, 1955, 1947, 1951, 1959, 1975, 1956, 1963,…\n$ race       <chr> \"White\", \"White\", \"White\", \"White\", \"AfricanAmerican\", \"Whi…\n$ latinx     <chr> \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ ethnic     <chr> \"White\", \"White\", \"Latinx\", \"White\", \"AfricanAmerican\", \"Wh…\n$ educate    <dbl> 14, 10, 16, 16, 18, 16, 13, 12, 8, 12, 19, 14, 13, 16, 12, …\n$ childs     <dbl> 0, 3, 2, 2, 0, 2, 6, 0, 4, 2, 2, 3, 2, 2, 2, 4, 0, 2, 2, 0,…\n$ marital    <dbl> 4, 3, 1, 1, 3, 2, 3, 4, 2, 1, 3, 3, 3, 1, 4, 2, 1, 1, 3, 4,…\n$ fincome    <dbl> 11, 12, 12, 13, 10, 10, 10, 12, 5, 12, 12, 11, 11, 12, 2, 1…\n$ pincome    <dbl> 11, 0, 22, 23, 0, 0, 12, 17, 2, 22, 23, 12, 0, 22, 0, 9, 20…\n$ sei        <dbl> 65.30, 14.80, 83.40, 69.30, 68.60, 69.30, 24.20, 23.70, 21.…\n$ occprest   <dbl> 47, 22, 61, 59, 53, 53, 48, 35, 35, 39, 72, 35, 45, 72, 28,…\n$ attend     <dbl> 5, 2, 2, 6, 8, 4, 7, 7, 0, 2, 4, 5, 0, 3, 0, 7, 1, 0, 4, 5,…\n$ relig      <dbl> 1, 2, 6, 1, 2, 2, 1, 2, 6, 1, 2, 1, 2, 2, 6, 2, 6, 4, 1, 2,…\n$ fund       <chr> \"moderate\", \"moderate\", \"liberal\", \"liberal\", \"moderate\", \"…\n$ owngun     <chr> \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no…\n$ legalmarij <chr> NA, \"no\", \"yes\", \"no\", \"no\", NA, \"yes\", \"yes\", \"yes\", \"yes\"…\n$ cappunish  <chr> \"yes\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\",…\n$ partyaff   <dbl> 6, 3, 5, 3, 7, 3, 1, 6, 4, 2, 7, 2, 2, 1, 5, 4, 3, 4, 2, 5,…\n$ polviews   <dbl> 6, 4, 5, 4, 7, 3, 4, 5, 4, 4, 6, 4, 4, 3, 2, 5, 2, 6, 2, 4,…\n$ spanking   <dbl> 2, NA, 2, 3, NA, 3, NA, NA, 1, 2, 3, NA, 3, NA, 3, 2, 3, 3,…\n$ lifesatis  <dbl> NA, 87.91, NA, 78.23, 77.39, NA, 72.31, 80.96, NA, 71.21, N…\n$ volunteer  <dbl> 1, 1, 1, 3, 3, 1, 3, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 5, 1,…\n$ confidence <dbl> 0, 10, 3, 3, 7, 1, 4, 2, 1, 4, 4, 1, 1, 3, 3, 1, 0, 3, 2, 3…\n$ civliberty <dbl> 12, 11, 0, 0, 12, 12, 10, 6, 0, 0, 12, 0, 5, 4, 0, 9, 0, 3,…\n$ watchtv    <dbl> 3, NA, 1, 1, NA, 8, NA, NA, 4, 2, 3, 3, 7, NA, 7, 5, 3, 1, …\n\n\n\n\n2.4.2.4 skimr(): Describe focal variables\n\nlibrary(skimr)\n\ngss2018 %>% \n  select(pincome, female) %>%\n  skim()  \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2315\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nfemale\n0\n1\n4\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npincome\n0\n1\n9.42\n8.95\n0\n0\n9\n18\n26\n▇▂▂▅▂\n\n\n\n\n\n\n\n2.4.2.5 psyc::describe(): Describe focal variables\n\nlibrary(skimr)\n\ngss2018 %>% \n  select(pincome, female) %>%\n  psych::describe()  \n\n        vars    n mean   sd median trimmed   mad min max range skew kurtosis\npincome    1 2315 9.42 8.95      9    8.85 13.34   0  26    26 0.21    -1.55\nfemale*    2 2315 1.45 0.50      1    1.44  0.00   1   2     1 0.20    -1.96\n          se\npincome 0.19\nfemale* 0.01\n\n\n\n\n2.4.2.6 Standard t.test\n\nt.test(gss2018$pincome ~ gss2018$female )\n\n\n    Welch Two Sample t-test\n\ndata:  gss2018$pincome by gss2018$female\nt = -7.1248, df = 2123.5, p-value = 1.422e-12\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -3.392792 -1.928197\nsample estimates:\nmean in group female   mean in group male \n             8.22135             10.88184 \n\n\n\n\n2.4.2.7 apaText t.test\n\nlibrary(apaText)\n\n# This code provides markdown text for Quarto documents\n\ngss2018 %>%\n  mutate(female = as.factor(female)) %>%\n  apa.ind.t.test(female, pincome, var.equal = FALSE)\n\n[1] \"$\\\\Delta M$ = 2.66, 95% CI[1.93, 3.39], *t*(2123.48) = 7.12, *p* < .001\""
  },
  {
    "objectID": "chapter3.html#page-40",
    "href": "chapter3.html#page-40",
    "title": "3  Chapter 3",
    "section": "3.1 Page 40",
    "text": "3.1 Page 40\n\n3.1.1 Nations Graph\n\n3.1.1.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n3.1.1.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/Nations2018.csv\",\n                save_as = \"nations2018.csv\")\n\n\n\n3.1.1.3 Load data from your computer\n\nnations2018 <- read_csv(\"nations2018.csv\") %>% \n  clean_names()\n\n\n\n3.1.1.4 Inspect data\n\nnations2018 %>% \n  glimpse()  \n\nRows: 8\nColumns: 4\n$ nation   <chr> \"Canada\", \"Finland\", \"France\", \"Germany\", \"Italy\", \"Japan\", \"…\n$ expend   <dbl> 21.0, 22.7, 23.4, 19.9, 19.0, 19.7, 18.5, 14.1\n$ econopen <dbl> 64.5, 76.2, 62.7, 87.4, 59.1, 34.6, 60.8, 27.1\n$ perlabor <dbl> 25.9, 60.3, 8.8, 16.5, 34.4, 17.0, 23.4, 10.1\n\n\n\n\n\n\n\nnation\nexpend\neconopen\nperlabor\n\n\n\n\nCanada\n21.0\n64.5\n25.9\n\n\nFinland\n22.7\n76.2\n60.3\n\n\nFrance\n23.4\n62.7\n8.8\n\n\nGermany\n19.9\n87.4\n16.5\n\n\nItaly\n19.0\n59.1\n34.4\n\n\nJapan\n19.7\n34.6\n17.0\n\n\nUnited Kingdom\n18.5\n60.8\n23.4\n\n\nUnited States\n14.1\n27.1\n10.1\n\n\n\n\n\n\n\n3.1.1.5 Graph\n\nnations_plot <- ggplot(data = nations2018,\n                        mapping = aes(x = perlabor,\n                                      y = expend)) +\n  geom_point(shape = 18) +\n  geom_text(mapping = aes(label = nation),\n            nudge_y = .4) +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"red\") +\n  coord_cartesian(xlim = c(5, 65),\n                  ylim = c(14, 24)) +\n  scale_x_continuous(breaks = seq(5, 65, by = 10)) +\n  scale_y_continuous(breaks = seq(14, 24, by = 2)) +\n  labs(x = \"Percent labor union\",\n       y = \"Public expenditures\",\n       title = \"Public Expenditures vs Percent Labor Union\")\n\n  \nprint(nations_plot)\n\n`geom_smooth()` using formula = 'y ~ x'"
  }
]