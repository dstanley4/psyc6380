[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSYC 6380 Psychological Applications of Multivariate Analysis",
    "section": "",
    "text": "Preface\nThis is a Quarto book designed to make it easier use R code with the textbook examples. In some cases I provide extra R code to make life easier for you (e.g., obtaining data from GitHub). In other cases, I use different code from the book to maintain the code-style we use in the course (i.e., a tidyverse approach to R)."
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Chapter 1",
    "section": "",
    "text": "No R-code in this chapter."
  },
  {
    "objectID": "chapter2.html#page-12",
    "href": "chapter2.html#page-12",
    "title": "2  Chapter 2",
    "section": "2.1 Page 12",
    "text": "2.1 Page 12\n\n2.1.1 Normal Distribution\n\nlibrary(tidyverse)\n\npopulation_data <- data.frame(weights = rnorm(1000000, 80, 10))\n\nweight_graph <- ggplot(data = population_data,\n                   mapping = aes(x = weights)) +\n  geom_density() +\n  scale_x_continuous(name = \"Distribution of Weights\") +\n  scale_y_continuous(name = \"Density\") +\n  theme_classic()\n\nprint(weight_graph)\n\n\n\n\n\n\n2.1.2 Skewed Distribution\n\nlibrary(tidyverse)\n\nincome_data <- data.frame(income = rf(1000000, df1 = 5, df2 = 2000))\n\nincome_graph <- ggplot(data = income_data,\n                   mapping = aes(x = income)) +\n  geom_density() +\n  scale_x_continuous(name = \"Distribution of Income\", \n                     breaks = seq(0,  6, by = 2)) +\n  scale_y_continuous(name = \"Density\",\n                     breaks = seq(0, .8, by = .2)) +\n\n  theme_classic()\n\nprint(income_graph)"
  },
  {
    "objectID": "chapter2.html#page-27",
    "href": "chapter2.html#page-27",
    "title": "2  Chapter 2",
    "section": "2.2 Page 27",
    "text": "2.2 Page 27\n\n2.2.1 Standardized Scores\n\nsample1_oz <- c(40, 45, 50, 55, 60, 65, 70)\nz_sample1_oz <- scale(sample1_oz, center = TRUE, scale = TRUE)\n\n\nsample2_grams <- c(1100, 1150, 1200, 1400, 1700, 1725, 1775)\nz_sample2_grams <- scale(sample2_grams, center = TRUE, scale = TRUE)\n\n\n\n\n\n\nsample1_oz\nsample2_grams\nz_sample1_oz\nz_sample2_grams\n\n\n\n\n40\n1100\n-1.3887301\n-1.1405606\n\n\n45\n1150\n-0.9258201\n-0.9706899\n\n\n50\n1200\n-0.4629100\n-0.8008191\n\n\n55\n1400\n0.0000000\n-0.1213362\n\n\n60\n1700\n0.4629100\n0.8978881\n\n\n65\n1725\n0.9258201\n0.9828235\n\n\n70\n1775\n1.3887301\n1.1526942"
  },
  {
    "objectID": "chapter2.html#page-30",
    "href": "chapter2.html#page-30",
    "title": "2  Chapter 2",
    "section": "2.3 Page 30",
    "text": "2.3 Page 30\n\n2.3.1 Correlation and Covariance\n\nsample1_oz <- c(40, 45, 50, 55, 60, 65, 70)\nsample1_length = c(31, 33, 37, 38, 42, 45, 48)\n\ncov_sample1 <- cov(sample1_oz, sample1_length)\nprint(cov_sample1)\n\n[1] 66.66667\n\ncorrelation_sample1 <- cor(sample1_oz, sample1_length)\nprint(correlation_sample1)\n\n[1] 0.9950372\n\n\nBut also note:\n\n# covariance is like correlation, but with the standard deviations included\n\ncorrelation_sample1 * sd(sample1_oz) * sd(sample1_length)\n\n[1] 66.66667\n\n# You get the same value as the covariance\nprint(cov_sample1)\n\n[1] 66.66667"
  },
  {
    "objectID": "chapter2.html#page-34",
    "href": "chapter2.html#page-34",
    "title": "2  Chapter 2",
    "section": "2.4 Page 34",
    "text": "2.4 Page 34\n\n2.4.1 Nations 2018\n\n2.4.1.1 Activate packages\n\nlibrary(usethis) # use_github_file() \nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \nlibrary(skimr) # skim()\n\n\n\n2.4.1.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/Nations2018.csv\",\n                save_as = \"nations2018.csv\")\n\n\n\n2.4.1.3 Load data from your computer\n\nnations2018 <- read_csv(\"nations2018.csv\") %>% \n  clean_names()\n\n\n\n2.4.1.4 Inspect data\n\nnations2018 %>% \n  glimpse()  \n\nRows: 8\nColumns: 4\n$ nation   <chr> \"Canada\", \"Finland\", \"France\", \"Germany\", \"Italy\", \"Japan\", \"…\n$ expend   <dbl> 21.0, 22.7, 23.4, 19.9, 19.0, 19.7, 18.5, 14.1\n$ econopen <dbl> 64.5, 76.2, 62.7, 87.4, 59.1, 34.6, 60.8, 27.1\n$ perlabor <dbl> 25.9, 60.3, 8.8, 16.5, 34.4, 17.0, 23.4, 10.1\n\n\nOr use\n\nnations2018 %>% \n  view()  \n\n\n\n\n\n\nnation\nexpend\neconopen\nperlabor\n\n\n\n\nCanada\n21.0\n64.5\n25.9\n\n\nFinland\n22.7\n76.2\n60.3\n\n\nFrance\n23.4\n62.7\n8.8\n\n\nGermany\n19.9\n87.4\n16.5\n\n\nItaly\n19.0\n59.1\n34.4\n\n\nJapan\n19.7\n34.6\n17.0\n\n\nUnited Kingdom\n18.5\n60.8\n23.4\n\n\nUnited States\n14.1\n27.1\n10.1\n\n\n\n\n\n\n\n2.4.1.5 Descriptive with skim()\n\nlibrary(skimr)\n\nnations2018 %>% \n  skim()  \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n8\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nnation\n0\n1\n5\n14\n0\n8\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nexpend\n0\n1\n19.79\n2.87\n14.1\n18.88\n19.80\n21.42\n23.4\n▂▁▅▇▅\n\n\neconopen\n0\n1\n59.05\n19.87\n27.1\n52.98\n61.75\n67.42\n87.4\n▅▁▇▂▅\n\n\nperlabor\n0\n1\n24.55\n16.72\n8.8\n14.90\n20.20\n28.02\n60.3\n▇▃▂▁▂\n\n\n\n\n\n\n\n2.4.1.6 Descriptives with describe()\nAlternatively, you could use the describe() command from the psych package as per the book. But, NEVER use library(psych) it will break the tidyverse. Instead use psyc:: before each psych package command.\n\n# psych package must be installed. But do not use library(psych)\n# Notice how psych creates a mean for the nation column - which makes no sense\nnations2018 %>% \n  psych::describe()  \n\n         vars n  mean    sd median trimmed   mad  min  max range  skew kurtosis\nnation*     1 8  4.50  2.45   4.50    4.50  2.97  1.0  8.0   7.0  0.00    -1.65\nexpend      2 8 19.79  2.87  19.80   19.79  1.85 14.1 23.4   9.3 -0.60    -0.62\neconopen    3 8 59.05 19.87  61.75   59.05 12.75 27.1 87.4  60.3 -0.31    -1.29\nperlabor    4 8 24.55 16.72  20.20   24.55 11.71  8.8 60.3  51.5  1.04    -0.19\n           se\nnation*  0.87\nexpend   1.01\neconopen 7.02\nperlabor 5.91\n\n\n\n\n\n2.4.2 GSS 2018\n\nlibrary(usethis) # use_github_file \nlibrary(tidyverse) # read_csv \nlibrary(janitor) # clean_names() \nlibrary(skimr) # skim\n\n\n2.4.2.1 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/GSS2018.csv\",\n                save_as = \"gss2018.csv\")\n\n\n\n2.4.2.2 Load data from your computer\n\ngss2018 <- read_csv(\"gss2018.csv\") %>% \n  clean_names()\n\n\n\n2.4.2.3 Inspect data\n\ngss2018 %>% \n  glimpse()  \n\nRows: 2,315\nColumns: 28\n$ id         <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ female     <chr> \"male\", \"female\", \"male\", \"female\", \"male\", \"female\", \"fema…\n$ age        <dbl> 43, 74, 42, 63, 71, 67, 59, 43, 62, 55, 59, 34, 61, 44, 41,…\n$ cohort     <dbl> 1975, 1944, 1976, 1955, 1947, 1951, 1959, 1975, 1956, 1963,…\n$ race       <chr> \"White\", \"White\", \"White\", \"White\", \"AfricanAmerican\", \"Whi…\n$ latinx     <chr> \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ ethnic     <chr> \"White\", \"White\", \"Latinx\", \"White\", \"AfricanAmerican\", \"Wh…\n$ educate    <dbl> 14, 10, 16, 16, 18, 16, 13, 12, 8, 12, 19, 14, 13, 16, 12, …\n$ childs     <dbl> 0, 3, 2, 2, 0, 2, 6, 0, 4, 2, 2, 3, 2, 2, 2, 4, 0, 2, 2, 0,…\n$ marital    <dbl> 4, 3, 1, 1, 3, 2, 3, 4, 2, 1, 3, 3, 3, 1, 4, 2, 1, 1, 3, 4,…\n$ fincome    <dbl> 11, 12, 12, 13, 10, 10, 10, 12, 5, 12, 12, 11, 11, 12, 2, 1…\n$ pincome    <dbl> 11, 0, 22, 23, 0, 0, 12, 17, 2, 22, 23, 12, 0, 22, 0, 9, 20…\n$ sei        <dbl> 65.30, 14.80, 83.40, 69.30, 68.60, 69.30, 24.20, 23.70, 21.…\n$ occprest   <dbl> 47, 22, 61, 59, 53, 53, 48, 35, 35, 39, 72, 35, 45, 72, 28,…\n$ attend     <dbl> 5, 2, 2, 6, 8, 4, 7, 7, 0, 2, 4, 5, 0, 3, 0, 7, 1, 0, 4, 5,…\n$ relig      <dbl> 1, 2, 6, 1, 2, 2, 1, 2, 6, 1, 2, 1, 2, 2, 6, 2, 6, 4, 1, 2,…\n$ fund       <chr> \"moderate\", \"moderate\", \"liberal\", \"liberal\", \"moderate\", \"…\n$ owngun     <chr> \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no…\n$ legalmarij <chr> NA, \"no\", \"yes\", \"no\", \"no\", NA, \"yes\", \"yes\", \"yes\", \"yes\"…\n$ cappunish  <chr> \"yes\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\",…\n$ partyaff   <dbl> 6, 3, 5, 3, 7, 3, 1, 6, 4, 2, 7, 2, 2, 1, 5, 4, 3, 4, 2, 5,…\n$ polviews   <dbl> 6, 4, 5, 4, 7, 3, 4, 5, 4, 4, 6, 4, 4, 3, 2, 5, 2, 6, 2, 4,…\n$ spanking   <dbl> 2, NA, 2, 3, NA, 3, NA, NA, 1, 2, 3, NA, 3, NA, 3, 2, 3, 3,…\n$ lifesatis  <dbl> NA, 87.91, NA, 78.23, 77.39, NA, 72.31, 80.96, NA, 71.21, N…\n$ volunteer  <dbl> 1, 1, 1, 3, 3, 1, 3, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 5, 1,…\n$ confidence <dbl> 0, 10, 3, 3, 7, 1, 4, 2, 1, 4, 4, 1, 1, 3, 3, 1, 0, 3, 2, 3…\n$ civliberty <dbl> 12, 11, 0, 0, 12, 12, 10, 6, 0, 0, 12, 0, 5, 4, 0, 9, 0, 3,…\n$ watchtv    <dbl> 3, NA, 1, 1, NA, 8, NA, NA, 4, 2, 3, 3, 7, NA, 7, 5, 3, 1, …\n\n\n\n\n2.4.2.4 skimr(): Describe focal variables\n\nlibrary(skimr)\n\ngss2018 %>% \n  select(pincome, female) %>%\n  skim()  \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2315\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nfemale\n0\n1\n4\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npincome\n0\n1\n9.42\n8.95\n0\n0\n9\n18\n26\n▇▂▂▅▂\n\n\n\n\n\n\n\n2.4.2.5 psyc::describe(): Describe focal variables\n\nlibrary(skimr)\n\ngss2018 %>% \n  select(pincome, female) %>%\n  psych::describe()  \n\n        vars    n mean   sd median trimmed   mad min max range skew kurtosis\npincome    1 2315 9.42 8.95      9    8.85 13.34   0  26    26 0.21    -1.55\nfemale*    2 2315 1.45 0.50      1    1.44  0.00   1   2     1 0.20    -1.96\n          se\npincome 0.19\nfemale* 0.01\n\n\n\n\n2.4.2.6 Standard t.test\n\nt.test(gss2018$pincome ~ gss2018$female )\n\n\n    Welch Two Sample t-test\n\ndata:  gss2018$pincome by gss2018$female\nt = -7.1248, df = 2123.5, p-value = 1.422e-12\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -3.392792 -1.928197\nsample estimates:\nmean in group female   mean in group male \n             8.22135             10.88184 \n\n\n\n\n2.4.2.7 apaText t.test\n\nlibrary(apaText)\n\n# This code provides markdown text for Quarto documents\n\ngss2018 %>%\n  mutate(female = as.factor(female)) %>%\n  apa.ind.t.test(female, pincome, var.equal = FALSE)\n\n[1] \"$\\\\Delta M$ = 2.66, 95% CI[1.93, 3.39], *t*(2123.48) = 7.12, *p* < .001\""
  },
  {
    "objectID": "chapter3.html#page-40",
    "href": "chapter3.html#page-40",
    "title": "3  Chapter 3",
    "section": "3.1 Page 40",
    "text": "3.1 Page 40\n\n3.1.1 Expenditures vs Labour Graph\n\n3.1.1.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n3.1.1.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/Nations2018.csv\",\n                save_as = \"nations2018.csv\")\n\n\n\n3.1.1.3 Load data from your computer\n\nnations2018 <- read_csv(\"nations2018.csv\") %>% \n  clean_names()\n\n\n\n3.1.1.4 Inspect data\n\nnations2018 %>% \n  glimpse()  \n\nRows: 8\nColumns: 4\n$ nation   <chr> \"Canada\", \"Finland\", \"France\", \"Germany\", \"Italy\", \"Japan\", \"…\n$ expend   <dbl> 21.0, 22.7, 23.4, 19.9, 19.0, 19.7, 18.5, 14.1\n$ econopen <dbl> 64.5, 76.2, 62.7, 87.4, 59.1, 34.6, 60.8, 27.1\n$ perlabor <dbl> 25.9, 60.3, 8.8, 16.5, 34.4, 17.0, 23.4, 10.1\n\n\n\n\n\n\n\nnation\nexpend\neconopen\nperlabor\n\n\n\n\nCanada\n21.0\n64.5\n25.9\n\n\nFinland\n22.7\n76.2\n60.3\n\n\nFrance\n23.4\n62.7\n8.8\n\n\nGermany\n19.9\n87.4\n16.5\n\n\nItaly\n19.0\n59.1\n34.4\n\n\nJapan\n19.7\n34.6\n17.0\n\n\nUnited Kingdom\n18.5\n60.8\n23.4\n\n\nUnited States\n14.1\n27.1\n10.1\n\n\n\n\n\n\n\n3.1.1.5 Graph\n\nnations_plot <- ggplot(data = nations2018,\n                        mapping = aes(x = perlabor,\n                                      y = expend)) +\n  geom_point(shape = 18) +\n  geom_text(mapping = aes(label = nation),\n            nudge_y = .4) +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"red\") +\n  coord_cartesian(xlim = c(5, 65),\n                  ylim = c(14, 24)) +\n  scale_x_continuous(breaks = seq(5, 65, by = 10)) +\n  scale_y_continuous(breaks = seq(14, 24, by = 2)) +\n  labs(x = \"Percent labor union\",\n       y = \"Public expenditures\",\n       title = \"Public Expenditures vs Percent Labor Union\")\n\n  \nprint(nations_plot)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "chapter3.html#page-44",
    "href": "chapter3.html#page-44",
    "title": "3  Chapter 3",
    "section": "3.2 Page 44",
    "text": "3.2 Page 44\n\n3.2.1 Opioid vs Satisfaction Analysis\n\n3.2.1.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n3.2.1.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/StateData2018.csv\",\n                save_as = \"statedata2018.csv\")\n\n\n\n3.2.1.3 Load data from your computer\nClean names is essential here. It makes sure all column names are lower case. They are not all lower case in the original data file.\n\nstatedata2018 <- read_csv(\"statedata2018.csv\") %>% \n  clean_names()\n\n\n\n3.2.1.4 Inspect data\nThere are so many column names in this data set that we do the glimpse a bit differently. That is, we sort the order of the columns alphabetically prior to doing the glimpse(). It affects only the display of the column names - not the structure of the data.\n\nstatedata2018 %>% \n  select(sort(names(statedata2018))) %>%\n  glimpse()  \n\nRows: 50\nColumns: 77\n$ aa_voted_percent           <dbl> 49.6, NA, 50.9, 41.6, 49.9, 31.0, 48.3, 55.…\n$ alc_disorder_past_year     <dbl> 4.51, 6.57, 5.49, 5.38, 5.51, 6.30, 6.10, 6…\n$ assault_rate               <dbl> 283.4, 440.2, 252.1, 346.0, 236.6, 192.8, 1…\n$ assoc_degree               <dbl> 8.2, 8.5, 8.4, 6.7, 7.7, 8.6, 7.5, 7.9, 9.7…\n$ bach_degree                <dbl> 15.4, 19.0, 18.1, 14.2, 20.6, 24.9, 21.9, 1…\n$ binge_alc_past_month       <dbl> 21.40, 24.20, 23.10, 19.49, 23.52, 26.97, 2…\n$ burglary_rate              <dbl> 819.0, 427.6, 647.1, 835.7, 522.3, 438.2, 3…\n$ census_division            <chr> \"EastSouthCentral\", \"Pacific\", \"Mountain\", …\n$ census_region              <chr> \"South\", \"West\", \"West\", \"South\", \"West\", \"…\n$ comm_supervision_rate      <dbl> 1591.14, 1522.95, 1503.95, 2222.08, 1088.59…\n$ community_supervision      <dbl> 60700, 8400, 84800, 51500, 333300, 90900, 4…\n$ conservative               <dbl> 26.00835, 19.27924, 20.40239, 25.04501, 17.…\n$ cost_living                <dbl> 89.3, 129.9, 97.0, 86.9, 151.7, 105.6, 127.…\n$ death_row                  <dbl> 177, 0, 120, 32, 727, 3, 0, 0, 348, 49, 0, …\n$ death_row_rate             <dbl> 4.33, 0.00, 2.18, 1.33, 3.59, 0.09, 0.00, 0…\n$ doctoral_degree            <dbl> 1.0, 1.2, 1.2, 0.9, 1.6, 1.6, 1.6, 1.8, 1.1…\n$ dom_mig_rate               <dbl> 1.17, -14.58, 11.61, 0.82, -3.95, 7.60, -6.…\n$ dom_migration              <dbl> 5718, -10752, 83240, 2475, -156068, 43293, …\n$ fips_code                  <dbl> 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 15, 16, 17…\n$ gross_state_product        <dbl> 211197, 51479, 326446, 122704, 2797601, 345…\n$ guns                       <dbl> 161641, 15824, 179738, 79841, 344622, 92435…\n$ guns_per_capita            <dbl> 33.15, 21.38, 25.61, 26.57, 8.71, 16.48, 22…\n$ health_exp_per_capita      <dbl> 7281, 11064, 6452, 7408, 7549, 6804, 9859, …\n$ illicit_disorder_past_year <dbl> 2.86, 3.69, 2.79, 2.80, 2.97, 3.43, 3.47, 3…\n$ illicit_drugs_past_month   <dbl> 8.67, 16.81, 10.77, 10.51, 13.11, 17.83, 12…\n$ infant_mortality_rate      <dbl> 9.03, 5.17, 5.32, 8.20, 4.22, 4.76, 4.78, 7…\n$ intern_mig_rate            <dbl> 0.68, 3.26, 2.00, 0.75, 2.98, 1.44, 4.62, 1…\n$ intern_migration           <dbl> 3344, 2401, 14335, 2260, 117797, 8207, 1649…\n$ larceny_rate               <dbl> 2149.5, 2096.4, 2289.1, 2313.5, 1527.4, 185…\n$ latinx_voted_percent       <dbl> 29.1, NA, 48.8, 20.8, 43.3, 43.9, 41.1, NA,…\n$ life_expectancy            <dbl> 75.0, 77.8, 79.2, 75.5, 81.0, 80.0, 80.7, 7…\n$ life_satis                 <dbl> 51.42162, 52.08266, 51.38469, 52.34836, 51.…\n$ masters_degree             <dbl> 6.9, 7.6, 7.8, 5.9, 8.4, 10.9, 12.1, 8.9, 7…\n$ med_hh_income              <dbl> 48486, 76715, 56213, 45726, 71228, 68811, 7…\n$ median_age                 <dbl> 39.0, 33.5, 37.5, 38.0, 36.4, 36.7, 40.9, 4…\n$ mental_illness_past_year   <dbl> 19.95, 20.32, 18.02, 20.73, 18.18, 19.86, 1…\n$ motor_vehicle_theft_rate   <dbl> 209.1, 236.0, 261.3, 188.8, 391.3, 234.8, 1…\n$ murder_ms_rate             <dbl> 5.7, 5.6, 4.7, 5.6, 4.4, 2.8, 2.4, 5.8, 5.8…\n$ opioid_od_death_rate       <dbl> 9.0, 13.9, 13.5, 6.5, 5.3, 10.0, 27.7, 27.8…\n$ pain_pill_past_year        <dbl> 4.53, 4.81, 4.27, 5.03, 4.30, 4.87, 4.21, 4…\n$ per_age0_18                <dbl> 24, 27, 24, 25, 24, 24, 22, 22, 21, 26, 23,…\n$ per_age19_25               <dbl> 9, 9, 9, 9, 9, 9, 8, 8, 8, 9, 7, 9, 9, 9, 9…\n$ per_age26_34               <dbl> 12, 13, 12, 12, 14, 14, 11, 12, 12, 12, 12,…\n$ per_age35_54               <dbl> 25, 26, 24, 25, 26, 26, 27, 25, 25, 27, 25,…\n$ per_age55_64               <dbl> 14, 13, 12, 13, 12, 13, 15, 14, 14, 12, 13,…\n$ per_age65plus              <dbl> 17, 12, 18, 17, 14, 14, 17, 19, 21, 14, 19,…\n$ per_am_ind_alaskan         <dbl> 1.0, 16.0, 4.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.…\n$ per_asian                  <dbl> 1, 6, 3, 2, 15, 3, 5, 4, 3, 4, 38, 2, 6, 2,…\n$ per_black                  <dbl> 26.0, 3.0, 4.0, 15.0, 5.0, 4.0, 10.0, 21.0,…\n$ per_cap_income             <dbl> 26846, 35874, 29265, 25635, 35021, 36415, 4…\n$ per_child_poverty          <dbl> 24, 13, 20, 24, 17, 12, 14, 17, 19, 20, 12,…\n$ per_latinx                 <dbl> 4, 7, 32, 8, 39, 22, 17, 9, 26, 10, 10, 13,…\n$ per_pop_change2010_18      <dbl> 2.254, 3.828, 12.192, 3.354, 6.181, 13.247,…\n$ per_poverty                <dbl> 16.89, 11.11, 14.86, 16.40, 13.30, 10.29, 9…\n$ per_white                  <dbl> 66, 60, 54, 72, 37, 68, 66, 62, 53, 52, 21,…\n$ percent_uninsured          <dbl> 12.0, 14.3, 12.7, 9.8, 8.3, 8.6, 6.2, 6.8, …\n$ pop_change2010_18          <dbl> 107733, 27189, 779358, 97797, 2302522, 6662…\n$ pop_density                <dbl> 95.4, 1.3, 58.3, 56.9, 246.1, 50.8, 742.6, …\n$ pop18and_older             <dbl> 3814879, 551562, 5638481, 2317649, 30617582…\n$ population                 <dbl> 4887871, 737438, 7171646, 3013825, 39557045…\n$ prison_rate                <dbl> 1072.12, 797.73, 975.44, 1035.53, 662.04, 7…\n$ prisoners                  <dbl> 40900, 4400, 55000, 24000, 202700, 32100, 1…\n$ professional_degree        <dbl> 1.5, 1.8, 1.8, 1.3, 2.4, 2.3, 3.0, 1.9, 2.1…\n$ prop_crime_rate            <dbl> 3177.6, 2760.0, 3197.5, 3338.0, 2441.1, 253…\n$ rape_rate                  <dbl> 41.3, 104.7, 50.2, 59.4, 29.7, 56.7, 21.7, …\n$ religious                  <dbl> 32.03850, 24.12306, 26.40519, 30.05092, 24.…\n$ robbery_rate               <dbl> 96.9, 85.4, 92.8, 69.1, 125.5, 56.7, 87.8, …\n$ state                      <chr> \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\",…\n$ state_taxes_per_capita     <dbl> 2262, 2226, 2272, 3266, 4424, 2599, 5341, 4…\n$ suic_rate_age_adj          <dbl> 16.6, 27.0, 18.2, 20.8, 10.5, 20.3, 10.5, 1…\n$ suicide_idea_past_year     <dbl> 3.89, 5.34, 4.02, 4.59, 4.04, 5.41, 4.05, 4…\n$ suicide_rate               <dbl> 15.90, 28.07, 19.37, 21.72, 11.48, 21.59, 1…\n$ tobacco_past_month         <dbl> 30.12, 26.68, 21.19, 30.69, 16.23, 20.15, 2…\n$ total_voted_percent        <dbl> 69.0, 67.7, 68.6, 58.5, 61.5, 65.6, 68.0, 6…\n$ unemploy_rate              <dbl> 4.1, 6.5, 4.7, 3.5, 4.1, 3.1, 4.2, 4.0, 3.5…\n$ violent_crime_rate         <dbl> 427.4, 635.8, 399.9, 480.1, 396.1, 309.1, 2…\n$ white_voted_percent        <dbl> 52.6, 60.3, 65.3, 44.0, 61.4, 62.6, 57.7, 5…\n\n\n\n\n3.2.1.5 Graph\n\nstate_plot <- ggplot(data = statedata2018,\n                     mapping = aes(x = life_satis,\n                                   y = opioid_od_death_rate)) +\n  geom_point(shape = 1) +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"red\") +\n  coord_cartesian(xlim = c(49, 55),\n                  ylim = c(0, 50)) +\n  scale_x_continuous(breaks = seq(49, 55, by = 1)) +\n  scale_y_continuous(breaks = seq(10, 50, by = 10)) +\n  labs(x = \"Average Life Satisfaction\",\n       y = \"Opioid overdose deaths per 100,000\",\n       title = \"Opioid Deaths vs Life Satisfaction\") +\n  theme_light()\n\nprint(state_plot)\n\n`geom_smooth()` using formula = 'y ~ x'"
  }
]