[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSYC 6380 Psychological Applications of Multivariate Analysis",
    "section": "",
    "text": "Preface\nThis is a Quarto book designed to supplement PSYC 6380."
  },
  {
    "objectID": "oneway-reg-anova.html#coding-types",
    "href": "oneway-reg-anova.html#coding-types",
    "title": "1  One-way ANOVA via Regression",
    "section": "1.1 Coding Types",
    "text": "1.1 Coding Types\n\n\n\n\n\n\n\n\n\nName/Synonym\nR command Example\nNature of Comparison\nNote\n\n\n\n\nTreatment Contrast / Dummy Contrast\n\nIntercept is the reference group mean. Unstandardized weights indicate difference between a group’s mean and reference group mean. In this example, group 1 is the reference group.\nDefault in R. Do NOT use for ANOVA.\n\n\nSum Contrast / Effect Contrast\n\nIntercept is the grand mean. Unstandardized weights indicate difference between a group’s mean and last group’s mean (Group 4 here).\nDo NOT use for ANOVA.\n\n\nHelmert Contrast\n\nIntercept is the grand mean. First contrast, unstandardized weight indicate difference between Group 2 and Group 1 means. Second contrast, unstandardized weight indicate difference between Group 3 mean and the average of the Group 1 and Group 2 means. Third contrast, unstandardized weight indicate difference between Group 4 mean and the average of the Group 1, Group 2, and Group 3 means. And so on.\nUSE for ANOVA."
  },
  {
    "objectID": "oneway-reg-anova.html#treatementdummy-coding",
    "href": "oneway-reg-anova.html#treatementdummy-coding",
    "title": "1  One-way ANOVA via Regression",
    "section": "1.2 Treatement/Dummy Coding",
    "text": "1.2 Treatement/Dummy Coding\nUse the Treatment contrast ONLY when you are interested in the contrast itself. DO NOT use if you are interested in typical ANOVA results (main effect, main effect, interaction, etc.).\nThis approach is the default appraoch in R unless you specify otherwise. In most cases, this is NOT what you want in Psychology analyses.\nComparisons are to one specific level of the Independent Variables that we call the reference group.\n\n1.2.1 Original Data\n\nprint(viagra)\n\n   libido      dose\n1       3   placebo\n2       2   placebo\n3       1   placebo\n4       1   placebo\n5       4   placebo\n6       5  low_dose\n7       2  low_dose\n8       4  low_dose\n9       2  low_dose\n10      3  low_dose\n11      7 high_dose\n12      4 high_dose\n13      5 high_dose\n14      3 high_dose\n15      6 high_dose\n\n\nNote the means for the three groups are:\n\nviagra %&gt;% group_by(dose) %&gt;% summarise(group_mean = mean(libido))\n\n# A tibble: 3 × 2\n  dose      group_mean\n  &lt;fct&gt;          &lt;dbl&gt;\n1 placebo          2.2\n2 low_dose         3.2\n3 high_dose        5  \n\n\n\n\n1.2.2 Set Factor with Reference Group\n\nviagra &lt;- viagra %&gt;%\n  mutate(dose = as_factor(dose)) %&gt;%\n  mutate(dose = relevel(dose, ref = \"placebo\"))\n\n\n\n1.2.3 Regression with Treatment Contrast\nThe computer will always use contrasts when there are categorical variables. So you should set the contrast you want. Here we set the contrast as Treatment (or Dummy) Coding. In most cases you will not want Treatment Coding. You will want Sum Coding we cover that in the next section. But we start with Treament Coding because it is easier to understand.\n\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))\n\nlm_viagra &lt;- lm(libido ~ dose, data = viagra)\n\n\ntidy(lm_viagra)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.2\n0.6271629\n3.507860\n0.0043189\n\n\ndoselow_dose\n1.0\n0.8869423\n1.127469\n0.2815839\n\n\ndosehigh_dose\n2.8\n0.8869423\n3.156913\n0.0082681\n\n\n\n\n\nWhat is going on here? The single dose column has disappeared. Instead we get \\(b\\)-weights for doselow_dose and dosehigh_dose. How do you interpret that information?\n\n\n1.2.4 Treatment Contrasts Explained\nIn the above analysis, we take a categorical variable and indicate it’s factor with the code below. With this code we are telling the computer it as a factor (i.e., a categorical variable) and indicating, with ref = “placebo”, that all the group means should be compared to the placebo group mean - if a treatment contrast is used.\n\nviagra &lt;- viagra %&gt;%\n  mutate(dose = as_factor(dose)) %&gt;%\n  mutate(dose = relevel(dose, ref = \"placebo\"))\n\nIn conjuction with the above “as_factor” commmand, we specify the rule for turning the factor (i.e., categorical variables) into “contrast columns” that will be used for the actual analysis. We do that with the line below that indicates we want to use treatment coding also known as dummy coding. It is CRITICAL that you set the contrast used for your regression if you have categorical variables.\n\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))\n\nA contrast will ALWAYS be used if you have categorical variables - you want to make sure it’s the one you want. Treatment Contrast is probably not the one you want in most cases - but we start with this one because it is common and easy to understand.\nThe combination of the two code blocks above (creating a factor, setting the contrast) results in the computer creating a data set like the one below when you conduct the regression.\n\nprint(viagra_dummy_coded)\n\n# A tibble: 15 × 3\n   libido dose_low_dose dose_high_dose\n    &lt;int&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n 1      3             0              0\n 2      2             0              0\n 3      1             0              0\n 4      1             0              0\n 5      4             0              0\n 6      5             1              0\n 7      2             1              0\n 8      4             1              0\n 9      2             1              0\n10      3             1              0\n11      7             0              1\n12      4             0              1\n13      5             0              1\n14      3             0              1\n15      6             0              1\n\n\nThen, when we specified this:\n\nlm_viagra &lt;- lm(libido ~ dose, data = viagra)\n\nThe computer actually ran the code below. Notice how the predictors are the contrast columns dose_low_dose and dose_high_dose. That’s how the computer handles categorical variables in a regression.\n\nlm_viagra &lt;- lm(libido ~ dose_low_dose + dose_high_dose,\n                data = viagra_dummy_coded)\n\n\ntidy(lm_viagra)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.2\n0.6271629\n3.507860\n0.0043189\n\n\ndose_low_dose\n1.0\n0.8869423\n1.127469\n0.2815839\n\n\ndose_high_dose\n2.8\n0.8869423\n3.156913\n0.0082681\n\n\n\n\n\nThe \\(p\\)-values for the \\(b\\)-weights assess the statistical difference between each group and the reference group (i.e., placebo). So Treatment (Dummy) Contrasts are a great way to compare each group mean to the reference group.\nExamine the weights in the above table and see how they can be used to recreate the group means.\n\n\n\n\n\n\n\n1.2.5 ANOVA Summary Information\nWith a one-way ANOVA, it’s easy to exact ANOVA information from the Regression output.\n\nglance(lm_viagra)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.4603659\n0.3704268\n1.402379\n5.118644\n0.0246943\n2\n-24.68305\n57.3661\n60.1983\n23.6\n12\n15\n\n\n\n\n\nFrom this output you can see that for this one-way ANOVA, \\(F\\)(2, 12) = 5.119, \\(p\\) = .025. In a one-way ANOVA the effect size is \\(\\eta^2 = \\eta_{partial}^2=R^2= .46\\). Note that in a one-way ANOVA, \\(\\eta^2\\) = \\(\\eta_{partial}^2\\) but this is not the case when you move to N-way ANOVA.\n\nsummary(lm_viagra)\n\n\nCall:\nlm(formula = libido ~ dose_low_dose + dose_high_dose, data = viagra_dummy_coded)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -2.0   -1.2   -0.2    0.9    2.0 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)      2.2000     0.6272   3.508  0.00432 **\ndose_low_dose    1.0000     0.8869   1.127  0.28158   \ndose_high_dose   2.8000     0.8869   3.157  0.00827 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.402 on 12 degrees of freedom\nMultiple R-squared:  0.4604,    Adjusted R-squared:  0.3704 \nF-statistic: 5.119 on 2 and 12 DF,  p-value: 0.02469\n\n\nFrom this output you can (AGAIN) see that for this one-way ANOVA, \\(F\\)(2, 12) = 5.119, \\(p\\) = .025. In a one-way ANOVA the effect size is \\(\\eta^2 = \\eta_{partial}^2=R^2= .46\\).\nNote: A good exam question would be to present a table like this an then ask you the mean for each group. With treatment/dummy coding the regression weight indicate for the reference group the mean of that group. For the other groups, the regression weights indicate the difference from the reference group."
  },
  {
    "objectID": "oneway-reg-anova.html#sum-contrast-effect-contrast",
    "href": "oneway-reg-anova.html#sum-contrast-effect-contrast",
    "title": "1  One-way ANOVA via Regression",
    "section": "1.3 Sum Contrast / Effect Contrast",
    "text": "1.3 Sum Contrast / Effect Contrast\nUse the Sum contrast ONLY when you are interested in the contrast itself. DO NOT use if you are interested in typical ANOVA results (main effect, main effect, interaction, etc.).\nWith sum coding, the contrasts create \\(b\\)-weight represent comparisons of each group mean to the to the grand mean. What is the grand mean? It’s just the mean of the dependent variable column across all conditions.\n\nsummary_stat = viagra %&gt;%\n  summarise(grand_mean = mean(libido))\n\nprint(summary_stat)\n\n  grand_mean\n1   3.466667\n\n\nYou can see the grand mean is 3.4666667. This is just the mean of everyone in the experiment.\n\nviagra_sum_coded &lt;- viagra %&gt;% select(libido)\n\ndose1 &lt;- c(rep(1,5), rep(0,5), rep(-1,5))\ndose2 &lt;- c(rep(0,5), rep(1,5), rep(-1,5))\n\nviagra_sum_coded$dose1 &lt;- dose1\nviagra_sum_coded$dose2 &lt;- dose2\n\nreadr::write_csv(viagra_sum_coded, file = \"viagra_sum_coded.csv\")\n\n\n1.3.1 Behind the scenes\nIn the above analysis, when we used this code block:\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\n\nviagra &lt;- viagra %&gt;%\n  mutate(dose = as_factor(dose)) \n\nWe were creating the data set below:\n\nprint(viagra_sum_coded)\n\n   libido dose1 dose2\n1       3     1     0\n2       2     1     0\n3       1     1     0\n4       1     1     0\n5       4     1     0\n6       5     0     1\n7       2     0     1\n8       4     0     1\n9       2     0     1\n10      3     0     1\n11      7    -1    -1\n12      4    -1    -1\n13      5    -1    -1\n14      3    -1    -1\n15      6    -1    -1\n\n\nAbove, when we specified this:\n\nlm_viagra &lt;- lm(libido ~ dose, data = viagra)\n\nThe computer actually ran this:\n\nlm_viagra_sum &lt;- lm(libido ~ dose1 + dose2,\n                data = viagra_sum_coded)\n\n\ntidy(lm_viagra_sum)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n3.4666667\n0.3620927\n9.5739760\n0.0000006\n\n\ndose1\n-1.2666667\n0.5120764\n-2.4735893\n0.0293002\n\n\ndose2\n-0.2666667\n0.5120764\n-0.5207556\n0.6120112\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 ANOVA values\n\nglance(lm_viagra_sum)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.4603659\n0.3704268\n1.402379\n5.118644\n0.0246943\n2\n-24.68305\n57.3661\n60.1983\n23.6\n12\n15\n\n\n\n\n\nFrom this output you can see that for this one-way ANOVA, F(2,12) = 5.118644, p = 0.0246943.\n\nsummary(lm_viagra_sum)\n\n\nCall:\nlm(formula = libido ~ dose1 + dose2, data = viagra_sum_coded)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -2.0   -1.2   -0.2    0.9    2.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.4667     0.3621   9.574 5.72e-07 ***\ndose1        -1.2667     0.5121  -2.474   0.0293 *  \ndose2        -0.2667     0.5121  -0.521   0.6120    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.402 on 12 degrees of freedom\nMultiple R-squared:  0.4604,    Adjusted R-squared:  0.3704 \nF-statistic: 5.119 on 2 and 12 DF,  p-value: 0.02469\n\n\nFrom this output you can (again) see that for this one-way ANOVA, F(2,12) = 5.118644, p = 0.0246943.\nNote: A good exam question would be to present a table like this an then ask you the mean for each group. With treatment/dummy coding the regression weight indicate for the reference group the mean of that group. For the other groups, the regression weights indicate the difference from the reference group."
  },
  {
    "objectID": "oneway-reg-anova.html#helmert-contrast",
    "href": "oneway-reg-anova.html#helmert-contrast",
    "title": "1  One-way ANOVA via Regression",
    "section": "1.4 Helmert Contrast",
    "text": "1.4 Helmert Contrast\nUse the Helmert Contrast if you are interested in typical ANOVA results (main effect, main effect, interaction, etc.).\nMore on this later."
  },
  {
    "objectID": "twoway-reg-anova.html#conducting-a-2-way-anova",
    "href": "twoway-reg-anova.html#conducting-a-2-way-anova",
    "title": "2  Two-way ANOVA via Regression",
    "section": "2.1 Conducting a 2-way ANOVA",
    "text": "2.1 Conducting a 2-way ANOVA\n\n2.1.1 Activate Packages\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(pracma)\nlibrary(recipes)\nlibrary(forcats)\nlibrary(tidymodels)\nlibrary(apaTables)\n\n\n\n2.1.2 Load Data\n\ngdata = read_csv(\"gdata.csv\")\n\n\n\n2.1.3 Inspect Data\n\nglimpse(gdata)\n\nRows: 41\nColumns: 3\n$ attractiveness &lt;dbl&gt; 60, 60, 55, 60, 55, 70, 65, 60, 70, 65, 60, 60, 50, 55,…\n$ gender         &lt;chr&gt; \"female\", \"female\", \"female\", \"female\", \"female\", \"fema…\n$ alcohol        &lt;chr&gt; \"none\", \"none\", \"none\", \"none\", \"none\", \"pint2\", \"pint2…\n\n\n\nprint(gdata)\n\n\n\n   attractiveness gender alcohol\n1              60 female    none\n2              60 female    none\n3              55 female    none\n4              60 female    none\n5              55 female    none\n6              70 female   pint2\n7              65 female   pint2\n8              60 female   pint2\n9              70 female   pint2\n10             65 female   pint2\n11             60 female   pint2\n12             60 female   pint2\n13             50 female   pint2\n14             55 female   pint4\n15             65 female   pint4\n16             70 female   pint4\n17             55 female   pint4\n18             55 female   pint4\n19             60 female   pint4\n20             50 female   pint4\n21             50 female   pint4\n22             50   male    none\n23             55   male    none\n24             80   male    none\n25             65   male    none\n26             70   male    none\n27             75   male    none\n28             75   male    none\n29             65   male    none\n30             45   male   pint2\n31             60   male   pint2\n32             85   male   pint2\n33             65   male   pint2\n34             70   male   pint2\n35             70   male   pint2\n36             80   male   pint2\n37             60   male   pint2\n38             30   male   pint4\n39             30   male   pint4\n40             30   male   pint4\n41             55   male   pint4\n\n\n\n\n2.1.4 Make Factors\n\ngdata &lt;- gdata %&gt;%\n  mutate(gender = as_factor(gender)) %&gt;%\n  mutate(alcohol = as_factor(alcohol))\n\nglimpse(gdata)\n\nRows: 41\nColumns: 3\n$ attractiveness &lt;dbl&gt; 60, 60, 55, 60, 55, 70, 65, 60, 70, 65, 60, 60, 50, 55,…\n$ gender         &lt;fct&gt; female, female, female, female, female, female, female,…\n$ alcohol        &lt;fct&gt; none, none, none, none, none, pint2, pint2, pint2, pint…\n\n\n\n\n2.1.5 Linear Model\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nlm_output &lt;- lm(attractiveness ~ gender*alcohol, data = gdata)\n\n\ntable1 &lt;- apa.aov.table(lm_output, table.number = 1)\napa.save(\"table1aov.doc\", table1)"
  },
  {
    "objectID": "twoway-reg-anova.html#regression-becoming-anova",
    "href": "twoway-reg-anova.html#regression-becoming-anova",
    "title": "2  Two-way ANOVA via Regression",
    "section": "2.2 Regression Becoming ANOVA",
    "text": "2.2 Regression Becoming ANOVA\nA bit of magic seems to happen in the above. We conduct a regression and then somehow get an ANOVA table out at the end. How does that work? The key is understanding that when we specify the regression with factors in it - we a really giving the computer a set of instruction and a starting point - rather than an actual analysis. The computer does a few things “under the hood”:\n\nFactors are turned into contrasts columns\nMultiple columns be be required for a single factor\nThe number of contrast columns required for a single factor column is equal to the number levels minus one.\nThe “actual analysis” is conducted using the contrast columns - not the factor column from your data set."
  },
  {
    "objectID": "twoway-reg-anova.html#contrasts-for-categorical-variables",
    "href": "twoway-reg-anova.html#contrasts-for-categorical-variables",
    "title": "2  Two-way ANOVA via Regression",
    "section": "2.3 Contrasts for Categorical Variables",
    "text": "2.3 Contrasts for Categorical Variables\nLet’s load a new data set that has some contrast columns created already.\n\ngdata &lt;- read_csv(\"gdata_contrasts.csv\")\n\n\n2.3.1 Gender Contrasts\nIn R when you use the line:\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\n\nIt effectively runs the contr.sum() command on each factor column, when a regression is run, and creates contrasts based on the number of levels of each factor. For example, the sex factor, with levels, causes the command below to be run.\n\ncontr.sum(2)\n\n  [,1]\n1    1\n2   -1\n\n\nThese rules are applied to the gender column. We create a new column called sex where this rule has been applied. In the output below, I have already applied this rule and put the result in the sex column. Normally this happens “under the hood” and you don’t see it. Notice how every female is coded 1 in the sex column where as males are coded -1 in the sex column; consistent with the contr.sum(2) command.\n\ngdata %&gt;%\n  select(attractiveness, gender, sex) %&gt;%\n  as.data.frame()\n\n   attractiveness gender sex\n1              60 female   1\n2              60 female   1\n3              55 female   1\n4              60 female   1\n5              55 female   1\n6              70 female   1\n7              65 female   1\n8              60 female   1\n9              70 female   1\n10             65 female   1\n11             60 female   1\n12             60 female   1\n13             50 female   1\n14             55 female   1\n15             65 female   1\n16             70 female   1\n17             55 female   1\n18             55 female   1\n19             60 female   1\n20             50 female   1\n21             50 female   1\n22             50   male  -1\n23             55   male  -1\n24             80   male  -1\n25             65   male  -1\n26             70   male  -1\n27             75   male  -1\n28             75   male  -1\n29             65   male  -1\n30             45   male  -1\n31             60   male  -1\n32             85   male  -1\n33             65   male  -1\n34             70   male  -1\n35             70   male  -1\n36             80   male  -1\n37             60   male  -1\n38             30   male  -1\n39             30   male  -1\n40             30   male  -1\n41             55   male  -1\n\n\n\n\n2.3.2 Alcohol Contrasts\n\ncontr.sum(3)\n\n  [,1] [,2]\n1    1    0\n2    0    1\n3   -1   -1\n\n\nIn the output below, I have already applied this rule and put the result in the alc1 and alc2 columns. Normally this happens “under the hood” and you don’t see it. Notice how every levels of alcohol are coded using this scheme; consistent with the contr.sum(3) command.\n\ngdata %&gt;%\n  select(attractiveness, alcohol, alc1, alc2) %&gt;%\n  as.data.frame()\n\n   attractiveness alcohol alc1 alc2\n1              60    none    1    0\n2              60    none    1    0\n3              55    none    1    0\n4              60    none    1    0\n5              55    none    1    0\n6              70   pint2    0    1\n7              65   pint2    0    1\n8              60   pint2    0    1\n9              70   pint2    0    1\n10             65   pint2    0    1\n11             60   pint2    0    1\n12             60   pint2    0    1\n13             50   pint2    0    1\n14             55   pint4   -1   -1\n15             65   pint4   -1   -1\n16             70   pint4   -1   -1\n17             55   pint4   -1   -1\n18             55   pint4   -1   -1\n19             60   pint4   -1   -1\n20             50   pint4   -1   -1\n21             50   pint4   -1   -1\n22             50    none    1    0\n23             55    none    1    0\n24             80    none    1    0\n25             65    none    1    0\n26             70    none    1    0\n27             75    none    1    0\n28             75    none    1    0\n29             65    none    1    0\n30             45   pint2    0    1\n31             60   pint2    0    1\n32             85   pint2    0    1\n33             65   pint2    0    1\n34             70   pint2    0    1\n35             70   pint2    0    1\n36             80   pint2    0    1\n37             60   pint2    0    1\n38             30   pint4   -1   -1\n39             30   pint4   -1   -1\n40             30   pint4   -1   -1\n41             55   pint4   -1   -1\n\n\n\n\n2.3.3 Interaction Contrasts\nWe also need contrasts for the interaction. We create the interaction contrasts by multiplying the columns for sex, alc1, and alc2. You can see how we do so in the code below.\n\ngdata &lt;- gdata %&gt;%\n  mutate(int1 = sex*alc1,\n         int2 = sex*alc2)\n\nNow check out the full coding off all predictors. When the regression is run, and the ANOVA results created, these are the columns that are actually analyzed.\n\ngdata %&gt;%\n  select(attractiveness, sex, alc1, alc2, int1, int2) %&gt;%\n  as.data.frame()\n\n   attractiveness sex alc1 alc2 int1 int2\n1              60   1    1    0    1    0\n2              60   1    1    0    1    0\n3              55   1    1    0    1    0\n4              60   1    1    0    1    0\n5              55   1    1    0    1    0\n6              70   1    0    1    0    1\n7              65   1    0    1    0    1\n8              60   1    0    1    0    1\n9              70   1    0    1    0    1\n10             65   1    0    1    0    1\n11             60   1    0    1    0    1\n12             60   1    0    1    0    1\n13             50   1    0    1    0    1\n14             55   1   -1   -1   -1   -1\n15             65   1   -1   -1   -1   -1\n16             70   1   -1   -1   -1   -1\n17             55   1   -1   -1   -1   -1\n18             55   1   -1   -1   -1   -1\n19             60   1   -1   -1   -1   -1\n20             50   1   -1   -1   -1   -1\n21             50   1   -1   -1   -1   -1\n22             50  -1    1    0   -1    0\n23             55  -1    1    0   -1    0\n24             80  -1    1    0   -1    0\n25             65  -1    1    0   -1    0\n26             70  -1    1    0   -1    0\n27             75  -1    1    0   -1    0\n28             75  -1    1    0   -1    0\n29             65  -1    1    0   -1    0\n30             45  -1    0    1    0   -1\n31             60  -1    0    1    0   -1\n32             85  -1    0    1    0   -1\n33             65  -1    0    1    0   -1\n34             70  -1    0    1    0   -1\n35             70  -1    0    1    0   -1\n36             80  -1    0    1    0   -1\n37             60  -1    0    1    0   -1\n38             30  -1   -1   -1    1    1\n39             30  -1   -1   -1    1    1\n40             30  -1   -1   -1    1    1\n41             55  -1   -1   -1    1    1"
  },
  {
    "objectID": "twoway-reg-anova.html#degrees-of-freedom",
    "href": "twoway-reg-anova.html#degrees-of-freedom",
    "title": "2  Two-way ANOVA via Regression",
    "section": "2.4 Degrees of Freedom",
    "text": "2.4 Degrees of Freedom\nWhen you look at the the columns in the above output notice the number of columns we use for each predictor corresponds the degrees of freedom for that predictor.\n\n\n\n\n\n\n\n\n\nPredictor\ndf\nNumber of contrast columns\nContrast column names\n\n\n\n\nsex\n\\(df_a = a-1 = 2 -1 = 1\\)\n1\nsex\n\n\nalcohol\n\\(df_b = b-1 = 3 -1 = 2\\)\n2\nalc1, alch2\n\n\nsex by alcohol\n\\(df_{int} = df_a * df_b = (a-1)(b-1)=1(2)=2\\)\n2\nint1, int2"
  },
  {
    "objectID": "twoway-reg-anova.html#regression-command-i.e.-lm-overview",
    "href": "twoway-reg-anova.html#regression-command-i.e.-lm-overview",
    "title": "2  Two-way ANOVA via Regression",
    "section": "2.5 Regression command (i.e., lm) overview",
    "text": "2.5 Regression command (i.e., lm) overview\nTo get ANOVA results that are consistent with what are typically used in psychology you need to 1) Specify the contr.sum() contrast 2) Calculate the Sum of Squares using the logic for Type III Sum of Squares\nWhen you run an ANOVA using the command:\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nlm_output &lt;- lm(attractiveness ~ gender*alcohol, data = gdata)\n\nR actually runs a whole series of regressions for you and combines them into the single output table you saw above. These models fall into two categories Full and Restricted Models."
  },
  {
    "objectID": "twoway-reg-anova.html#full-and-restricted-models",
    "href": "twoway-reg-anova.html#full-and-restricted-models",
    "title": "2  Two-way ANOVA via Regression",
    "section": "2.6 Full and Restricted Models",
    "text": "2.6 Full and Restricted Models\n\n2.6.1 Full Model\nFirst, the Full Model is run that includes all of the predictor columns:\n\nlm_full                   &lt;- lm(attractiveness ~ sex + alc1 + alc2 + int1 + int2, data = gdata)\n\n\n\n2.6.2 Restricted Models\nNext a series of restricted models are run that excluded an effect of interest for each restricted model.\n\nlm_restricted_no_sex         &lt;- lm(attractiveness ~       alc1 + alc2 + int1 + int2, data = gdata)\nlm_restricted_no_alcohol     &lt;- lm(attractiveness ~ sex +               int1 + int2, data = gdata)\nlm_restricted_no_interaction &lt;- lm(attractiveness ~ sex + alc1 + alc2              , data = gdata)\nlm_restricted_no_intercept   &lt;- lm(attractiveness ~ sex + alc1 + alc2 + int1 + int2 - 1, data = gdata)"
  },
  {
    "objectID": "twoway-reg-anova.html#logic-model-comparison",
    "href": "twoway-reg-anova.html#logic-model-comparison",
    "title": "2  Two-way ANOVA via Regression",
    "section": "2.7 Logic: Model Comparison",
    "text": "2.7 Logic: Model Comparison\nThe ANOVA table is created by comparing each of these restricted models to the full model. For example, to determine the main effect for gender we compare the model lm_restricted_no_sex to the model lm_full. If lm_full accounts for substantially more variance than lm_restricted_no_sex it is significant.\nThis is effectively identical to when we looked at comparing two regression models previous. Using that logic, we could just write the code:\n\n# try this, it works!\nlibrary(apaTables)\napa.reg.table(lm_restricted_no_sex, lm_restricted_all)\n\nThe result would tell us if the main effect of sex is significant. The logic of calculating things this way is the Type III Sum of Squares logic. HOWEVER, we don’t tend to use the output in the form provided in this type of table. Rather the output if reformatted to by consistent with the way ANOVA’s are typically presented. The next few sections show you how the table below is created:"
  },
  {
    "objectID": "twoway-reg-anova.html#explanation-1-comparing-models",
    "href": "twoway-reg-anova.html#explanation-1-comparing-models",
    "title": "2  Two-way ANOVA via Regression",
    "section": "2.8 Explanation 1: Comparing Models",
    "text": "2.8 Explanation 1: Comparing Models\n\n2.8.1 Sex\n\nanova(lm_restricted_no_sex, lm_full)\n\nAnalysis of Variance Table\n\nModel 1: attractiveness ~ alc1 + alc2 + int1 + int2\nModel 2: attractiveness ~ sex + alc1 + alc2 + int1 + int2\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     36 3059.9                           \n2     35 2992.5  1    67.368 0.7879 0.3808\n\n\nCompare the \\(F-\\) and \\(p\\)-values in the above output to those in the table below.\n\n\n\n\n\n\n\n2.8.2 Alcohol\n\nanova(lm_restricted_no_alcohol, lm_full)\n\nAnalysis of Variance Table\n\nModel 1: attractiveness ~ sex + int1 + int2\nModel 2: attractiveness ~ sex + alc1 + alc2 + int1 + int2\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     37 5223.3                                  \n2     35 2992.5  2    2230.8 13.045 5.843e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCompare the \\(F-\\) and \\(p\\)-values in the above output to those in the table below.\n\n\n\n\n\n\n\n2.8.3 Interaction\n\nanova(lm_restricted_no_interaction, lm_full)\n\nAnalysis of Variance Table\n\nModel 1: attractiveness ~ sex + alc1 + alc2\nModel 2: attractiveness ~ sex + alc1 + alc2 + int1 + int2\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     37 4501.2                                  \n2     35 2992.5  2    1508.7 8.8225 0.0007896 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCompare the \\(F-\\) and \\(p\\)-values in the above output to those in the table below."
  },
  {
    "objectID": "twoway-reg-anova.html#explanation-2-sum-of-squares-to-anova",
    "href": "twoway-reg-anova.html#explanation-2-sum-of-squares-to-anova",
    "title": "2  Two-way ANOVA via Regression",
    "section": "2.9 Explanation 2: Sum of Squares to ANOVA",
    "text": "2.9 Explanation 2: Sum of Squares to ANOVA\nNotice the first column of the ANOVA table above is the Sum of Squares column. How are those values calculated? Let’s consider the example of alcohol as a predictor. We want to determine the Sum of Squares for alcohol.\nWe begin by calculating predicted scores for the Full Model (lm_full):\n\\[\n\\hat{y}_{\\text{full}} = b_0 +b_1sex + b_2alc1 + b_3alc2 + b_4int1 + b_5int2\n\\]\nNext, we calculate the predicted scores for the alcohol restricted model (lm_restricted_no_alcohol) \\[\n\\hat{y}_{\\text{restricted no alcohol}}= b_0 +b_1sex + b_2int1 + b_3int2\n\\]\nThen we calculated the difference between these two sets of predicted scores:\n\\[\n\\hat{y}_{\\text{difference}} = \\hat{y}_{\\text{full}} - \\hat{y}_{\\text{restricted no alcohol}}\n\\]\nThen we square these values and add them up.\n\\[\nSS_{alcohol} =  \\sum \\hat{y}_{\\text{difference}}^2\n\\]\nWe follow this process below for each predictor (including the intercept).\n\n2.9.1 Intercept\n\n## Sum of squares intercept\nsum( ( predict(lm_full) - predict(lm_restricted_no_intercept) )^2 )\n\n[1] 127477.9\n\n\nCompare the Sum of Squares values in the above output to the one in the table below.\n\n\n\n\n\n\n\n2.9.2 Sex\n\n## Sum of squares sex\nsum( ( predict(lm_full) - predict(lm_restricted_no_sex) )^2 )\n\n[1] 67.36842\n\n\nCompare the Sum of Squares values in the above output to the one in the table below.\n\n\n\n\n\n\n\n2.9.3 Alcohol\n\n## Sum of squares alcohol\nsum( ( predict(lm_full) - predict(lm_restricted_no_alcohol) )^2 )\n\n[1] 2230.757\n\n\nCompare the Sum of Squares values in the above output to the one in the table below.\n\n\n\n\n\n\n\n2.9.4 Interaction\n\n## Sum of squares interaction\nsum( ( predict(lm_full) - predict(lm_restricted_no_interaction) )^2 )\n\n[1] 1508.651\n\n\nCompare the Sum of Squares values in the above output to the one in the table below.\n\n\n\n\n\n\n\n2.9.5 Error\n\n## Sum of squares error\nsum( lm_full$residuals^2 )\n\n[1] 2992.5\n\n\nCompare the Sum of Squares values in the above output to the one in the table below.\n\n\n\n\n\n\n\n2.9.6 SS to ANOVA\nBased on the above analyses we know the Sum of Squares and the degrees of freedom for everything. We can put that information in the table below.\n\n\n\n\n\n\n\n\n\n\n\nPredictor\n\\(SS\\)\n\\(df\\)\nMS\\(=\\frac{SS}{df}\\)\nF\\(=\\frac{MS}{MS_{error}}\\)\np\n\n\n\n\n(Intercept)\n127477.89\n1\n\n\n\n\n\nsex\n67.37\n1\n\n\n\n\n\nalcohol\n2230.76\n2\n\n\n\n\n\nsex by alcohol\n1508.65\n2\n\n\n\n\n\nError\n2992.5\n35\n\n\n\n\n\n\nA few hand calculations, and an \\(F\\) to \\(p\\)-value look-up table, provides us with the rest of the information we need:\n\n\n\n\n\n\n\n\n\n\n\nPredictor\n\\(SS\\)\n\\(df\\)\nMS\\(=\\frac{SS}{df}\\)\nF\\(=\\frac{MS}{MS_{error}}\\)\np\n\n\n\n\n(Intercept)\n127477.89\n1\n\\(\\frac{127477.89}{1}=127477.89\\)\n\\(\\frac{127477.89}{85.5}= 1490.97\\)\n&lt;.001\n\n\nsex\n67.37\n1\n\\(\\frac{67.37}{1}=67.37\\)\n\\(\\frac{67.37}{85.5}= 0.79\\)\n.381\n\n\nalcohol\n2230.76\n2\n\\(\\frac{2230.76}{2}=1115.38\\)\n\\(\\frac{1115.38}{85.5}=13.05\\)\n&lt;.001\n\n\nsex by alcohol\n1508.65\n2\n\\(\\frac{1508.65}{2}= 754.325\\)\n\\(\\frac{754.325}{85.5}=8.82\\)\n.001\n\n\nError\n2992.5\n35\n\\(\\frac{2992.5}{35}= 85.5\\)"
  }
]