[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSYC 6380 Psychological Applications of Multivariate Analysis",
    "section": "",
    "text": "Preface\nThis is a Quarto book designed to make it easier use R code with the textbook examples. In some cases I provide extra R code to make life easier for you (e.g., obtaining data from GitHub). In other cases, I use different code from the book to maintain the code-style we use in the course (i.e., a tidyverse approach to R)."
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Chapter 1",
    "section": "",
    "text": "No R-code in this chapter."
  },
  {
    "objectID": "chapter2.html#page-12",
    "href": "chapter2.html#page-12",
    "title": "2  Chapter 2",
    "section": "2.1 Page 12",
    "text": "2.1 Page 12\n\n2.1.1 Normal Distribution\n\nlibrary(tidyverse)\n\npopulation_data <- data.frame(weights = rnorm(1000000, 80, 10))\n\nweight_graph <- ggplot(data = population_data,\n                   mapping = aes(x = weights)) +\n  geom_density() +\n  scale_x_continuous(name = \"Distribution of Weights\") +\n  scale_y_continuous(name = \"Density\") +\n  theme_classic()\n\nprint(weight_graph)\n\n\n\n\n\n\n2.1.2 Skewed Distribution\n\nlibrary(tidyverse)\n\nincome_data <- data.frame(income = rf(1000000, df1 = 5, df2 = 2000))\n\nincome_graph <- ggplot(data = income_data,\n                   mapping = aes(x = income)) +\n  geom_density() +\n  scale_x_continuous(name = \"Distribution of Income\", \n                     breaks = seq(0,  6, by = 2)) +\n  scale_y_continuous(name = \"Density\",\n                     breaks = seq(0, .8, by = .2)) +\n\n  theme_classic()\n\nprint(income_graph)"
  },
  {
    "objectID": "chapter2.html#page-27",
    "href": "chapter2.html#page-27",
    "title": "2  Chapter 2",
    "section": "2.2 Page 27",
    "text": "2.2 Page 27\n\n2.2.1 Standardized Scores\n\nsample1_oz <- c(40, 45, 50, 55, 60, 65, 70)\nz_sample1_oz <- scale(sample1_oz, center = TRUE, scale = TRUE)\n\n\nsample2_grams <- c(1100, 1150, 1200, 1400, 1700, 1725, 1775)\nz_sample2_grams <- scale(sample2_grams, center = TRUE, scale = TRUE)\n\n\n\n\n\n\nsample1_oz\nsample2_grams\nz_sample1_oz\nz_sample2_grams\n\n\n\n\n40\n1100\n-1.3887301\n-1.1405606\n\n\n45\n1150\n-0.9258201\n-0.9706899\n\n\n50\n1200\n-0.4629100\n-0.8008191\n\n\n55\n1400\n0.0000000\n-0.1213362\n\n\n60\n1700\n0.4629100\n0.8978881\n\n\n65\n1725\n0.9258201\n0.9828235\n\n\n70\n1775\n1.3887301\n1.1526942"
  },
  {
    "objectID": "chapter2.html#page-30",
    "href": "chapter2.html#page-30",
    "title": "2  Chapter 2",
    "section": "2.3 Page 30",
    "text": "2.3 Page 30\n\n2.3.1 Correlation and Covariance\n\nsample1_oz <- c(40, 45, 50, 55, 60, 65, 70)\nsample1_length = c(31, 33, 37, 38, 42, 45, 48)\n\ncov_sample1 <- cov(sample1_oz, sample1_length)\nprint(cov_sample1)\n\n[1] 66.66667\n\ncorrelation_sample1 <- cor(sample1_oz, sample1_length)\nprint(correlation_sample1)\n\n[1] 0.9950372\n\n\nBut also note:\n\n# covariance is like correlation, but with the standard deviations included\n\ncorrelation_sample1 * sd(sample1_oz) * sd(sample1_length)\n\n[1] 66.66667\n\n# You get the same value as the covariance\nprint(cov_sample1)\n\n[1] 66.66667"
  },
  {
    "objectID": "chapter2.html#page-34",
    "href": "chapter2.html#page-34",
    "title": "2  Chapter 2",
    "section": "2.4 Page 34",
    "text": "2.4 Page 34\n\n2.4.1 Nations 2018\n\n2.4.1.1 Activate packages\n\nlibrary(usethis) # use_github_file() \nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \nlibrary(skimr) # skim()\n\n\n\n2.4.1.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/Nations2018.csv\",\n                save_as = \"nations2018.csv\")\n\n\n\n2.4.1.3 Load data from your computer\n\nnations2018 <- read_csv(\"nations2018.csv\") %>% \n  clean_names()\n\n\n\n2.4.1.4 Inspect data\n\nnations2018 %>% \n  glimpse()  \n\nRows: 8\nColumns: 4\n$ nation   <chr> \"Canada\", \"Finland\", \"France\", \"Germany\", \"Italy\", \"Japan\", \"…\n$ expend   <dbl> 21.0, 22.7, 23.4, 19.9, 19.0, 19.7, 18.5, 14.1\n$ econopen <dbl> 64.5, 76.2, 62.7, 87.4, 59.1, 34.6, 60.8, 27.1\n$ perlabor <dbl> 25.9, 60.3, 8.8, 16.5, 34.4, 17.0, 23.4, 10.1\n\n\nOr use\n\nnations2018 %>% \n  view()  \n\n\n\n\n\n\nnation\nexpend\neconopen\nperlabor\n\n\n\n\nCanada\n21.0\n64.5\n25.9\n\n\nFinland\n22.7\n76.2\n60.3\n\n\nFrance\n23.4\n62.7\n8.8\n\n\nGermany\n19.9\n87.4\n16.5\n\n\nItaly\n19.0\n59.1\n34.4\n\n\nJapan\n19.7\n34.6\n17.0\n\n\nUnited Kingdom\n18.5\n60.8\n23.4\n\n\nUnited States\n14.1\n27.1\n10.1\n\n\n\n\n\n\n\n2.4.1.5 Descriptive with skim()\n\nlibrary(skimr)\n\nnations2018 %>% \n  skim()  \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n8\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nnation\n0\n1\n5\n14\n0\n8\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nexpend\n0\n1\n19.79\n2.87\n14.1\n18.88\n19.80\n21.42\n23.4\n▂▁▅▇▅\n\n\neconopen\n0\n1\n59.05\n19.87\n27.1\n52.98\n61.75\n67.42\n87.4\n▅▁▇▂▅\n\n\nperlabor\n0\n1\n24.55\n16.72\n8.8\n14.90\n20.20\n28.02\n60.3\n▇▃▂▁▂\n\n\n\n\n\n\n\n2.4.1.6 Descriptives with describe()\nAlternatively, you could use the describe() command from the psych package as per the book. But, NEVER use library(psych) it will break the tidyverse. Instead use psyc:: before each psych package command.\n\n# psych package must be installed. But do not use library(psych)\n# Notice how psych creates a mean for the nation column - which makes no sense\nnations2018 %>% \n  psych::describe()  \n\n         vars n  mean    sd median trimmed   mad  min  max range  skew kurtosis\nnation*     1 8  4.50  2.45   4.50    4.50  2.97  1.0  8.0   7.0  0.00    -1.65\nexpend      2 8 19.79  2.87  19.80   19.79  1.85 14.1 23.4   9.3 -0.60    -0.62\neconopen    3 8 59.05 19.87  61.75   59.05 12.75 27.1 87.4  60.3 -0.31    -1.29\nperlabor    4 8 24.55 16.72  20.20   24.55 11.71  8.8 60.3  51.5  1.04    -0.19\n           se\nnation*  0.87\nexpend   1.01\neconopen 7.02\nperlabor 5.91\n\n\n\n\n\n2.4.2 GSS 2018\n\nlibrary(usethis) # use_github_file \nlibrary(tidyverse) # read_csv \nlibrary(janitor) # clean_names() \nlibrary(skimr) # skim\n\n\n2.4.2.1 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/GSS2018.csv\",\n                save_as = \"gss2018.csv\")\n\n\n\n2.4.2.2 Load data from your computer\n\ngss2018 <- read_csv(\"gss2018.csv\") %>% \n  clean_names()\n\n\n\n2.4.2.3 Inspect data\n\ngss2018 %>% \n  glimpse()  \n\nRows: 2,315\nColumns: 28\n$ id         <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ female     <chr> \"male\", \"female\", \"male\", \"female\", \"male\", \"female\", \"fema…\n$ age        <dbl> 43, 74, 42, 63, 71, 67, 59, 43, 62, 55, 59, 34, 61, 44, 41,…\n$ cohort     <dbl> 1975, 1944, 1976, 1955, 1947, 1951, 1959, 1975, 1956, 1963,…\n$ race       <chr> \"White\", \"White\", \"White\", \"White\", \"AfricanAmerican\", \"Whi…\n$ latinx     <chr> \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ ethnic     <chr> \"White\", \"White\", \"Latinx\", \"White\", \"AfricanAmerican\", \"Wh…\n$ educate    <dbl> 14, 10, 16, 16, 18, 16, 13, 12, 8, 12, 19, 14, 13, 16, 12, …\n$ childs     <dbl> 0, 3, 2, 2, 0, 2, 6, 0, 4, 2, 2, 3, 2, 2, 2, 4, 0, 2, 2, 0,…\n$ marital    <dbl> 4, 3, 1, 1, 3, 2, 3, 4, 2, 1, 3, 3, 3, 1, 4, 2, 1, 1, 3, 4,…\n$ fincome    <dbl> 11, 12, 12, 13, 10, 10, 10, 12, 5, 12, 12, 11, 11, 12, 2, 1…\n$ pincome    <dbl> 11, 0, 22, 23, 0, 0, 12, 17, 2, 22, 23, 12, 0, 22, 0, 9, 20…\n$ sei        <dbl> 65.30, 14.80, 83.40, 69.30, 68.60, 69.30, 24.20, 23.70, 21.…\n$ occprest   <dbl> 47, 22, 61, 59, 53, 53, 48, 35, 35, 39, 72, 35, 45, 72, 28,…\n$ attend     <dbl> 5, 2, 2, 6, 8, 4, 7, 7, 0, 2, 4, 5, 0, 3, 0, 7, 1, 0, 4, 5,…\n$ relig      <dbl> 1, 2, 6, 1, 2, 2, 1, 2, 6, 1, 2, 1, 2, 2, 6, 2, 6, 4, 1, 2,…\n$ fund       <chr> \"moderate\", \"moderate\", \"liberal\", \"liberal\", \"moderate\", \"…\n$ owngun     <chr> \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no…\n$ legalmarij <chr> NA, \"no\", \"yes\", \"no\", \"no\", NA, \"yes\", \"yes\", \"yes\", \"yes\"…\n$ cappunish  <chr> \"yes\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\",…\n$ partyaff   <dbl> 6, 3, 5, 3, 7, 3, 1, 6, 4, 2, 7, 2, 2, 1, 5, 4, 3, 4, 2, 5,…\n$ polviews   <dbl> 6, 4, 5, 4, 7, 3, 4, 5, 4, 4, 6, 4, 4, 3, 2, 5, 2, 6, 2, 4,…\n$ spanking   <dbl> 2, NA, 2, 3, NA, 3, NA, NA, 1, 2, 3, NA, 3, NA, 3, 2, 3, 3,…\n$ lifesatis  <dbl> NA, 87.91, NA, 78.23, 77.39, NA, 72.31, 80.96, NA, 71.21, N…\n$ volunteer  <dbl> 1, 1, 1, 3, 3, 1, 3, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 5, 1,…\n$ confidence <dbl> 0, 10, 3, 3, 7, 1, 4, 2, 1, 4, 4, 1, 1, 3, 3, 1, 0, 3, 2, 3…\n$ civliberty <dbl> 12, 11, 0, 0, 12, 12, 10, 6, 0, 0, 12, 0, 5, 4, 0, 9, 0, 3,…\n$ watchtv    <dbl> 3, NA, 1, 1, NA, 8, NA, NA, 4, 2, 3, 3, 7, NA, 7, 5, 3, 1, …\n\n\n\n\n2.4.2.4 skimr(): Describe focal variables\n\nlibrary(skimr)\n\ngss2018 %>% \n  select(pincome, female) %>%\n  skim()  \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2315\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nfemale\n0\n1\n4\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npincome\n0\n1\n9.42\n8.95\n0\n0\n9\n18\n26\n▇▂▂▅▂\n\n\n\n\n\n\n\n2.4.2.5 psyc::describe(): Describe focal variables\n\nlibrary(skimr)\n\ngss2018 %>% \n  select(pincome, female) %>%\n  psych::describe()  \n\n        vars    n mean   sd median trimmed   mad min max range skew kurtosis\npincome    1 2315 9.42 8.95      9    8.85 13.34   0  26    26 0.21    -1.55\nfemale*    2 2315 1.45 0.50      1    1.44  0.00   1   2     1 0.20    -1.96\n          se\npincome 0.19\nfemale* 0.01\n\n\n\n\n2.4.2.6 Standard t.test\n\nt.test(gss2018$pincome ~ gss2018$female )\n\n\n    Welch Two Sample t-test\n\ndata:  gss2018$pincome by gss2018$female\nt = -7.1248, df = 2123.5, p-value = 1.422e-12\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -3.392792 -1.928197\nsample estimates:\nmean in group female   mean in group male \n             8.22135             10.88184 \n\n\n\n\n2.4.2.7 apaText t.test\n\nlibrary(apaText)\n\n# This code provides markdown text for Quarto documents\n\ngss2018 %>%\n  mutate(female = as.factor(female)) %>%\n  apa.ind.t.test(female, pincome, var.equal = FALSE)\n\n[1] \"$\\\\Delta M$ = 2.66, 95% CI[1.93, 3.39], *t*(2123.48) = 7.12, *p* < .001\""
  },
  {
    "objectID": "chapter3.html#page-40",
    "href": "chapter3.html#page-40",
    "title": "3  Chapter 3",
    "section": "3.1 Page 40",
    "text": "3.1 Page 40\n\n3.1.1 Expenditures vs Labour Graph\n\n3.1.1.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n3.1.1.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/Nations2018.csv\",\n                save_as = \"nations2018.csv\")\n\n\n\n3.1.1.3 Load data from your computer\n\nnations2018 <- read_csv(\"nations2018.csv\") %>% \n  clean_names()\n\n\n\n3.1.1.4 Inspect data\n\nnations2018 %>% \n  glimpse()  \n\nRows: 8\nColumns: 4\n$ nation   <chr> \"Canada\", \"Finland\", \"France\", \"Germany\", \"Italy\", \"Japan\", \"…\n$ expend   <dbl> 21.0, 22.7, 23.4, 19.9, 19.0, 19.7, 18.5, 14.1\n$ econopen <dbl> 64.5, 76.2, 62.7, 87.4, 59.1, 34.6, 60.8, 27.1\n$ perlabor <dbl> 25.9, 60.3, 8.8, 16.5, 34.4, 17.0, 23.4, 10.1\n\n\n\n\n\n\n\nnation\nexpend\neconopen\nperlabor\n\n\n\n\nCanada\n21.0\n64.5\n25.9\n\n\nFinland\n22.7\n76.2\n60.3\n\n\nFrance\n23.4\n62.7\n8.8\n\n\nGermany\n19.9\n87.4\n16.5\n\n\nItaly\n19.0\n59.1\n34.4\n\n\nJapan\n19.7\n34.6\n17.0\n\n\nUnited Kingdom\n18.5\n60.8\n23.4\n\n\nUnited States\n14.1\n27.1\n10.1\n\n\n\n\n\n\n\n3.1.1.5 Graph\n\nnations_plot <- ggplot(data = nations2018,\n                        mapping = aes(x = perlabor,\n                                      y = expend)) +\n  geom_point(shape = 18) +\n  geom_text(mapping = aes(label = nation),\n            nudge_y = .4) +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"red\") +\n  coord_cartesian(xlim = c(5, 65),\n                  ylim = c(14, 24)) +\n  scale_x_continuous(breaks = seq(5, 65, by = 10)) +\n  scale_y_continuous(breaks = seq(14, 24, by = 2)) +\n  labs(x = \"Percent labor union\",\n       y = \"Public expenditures\",\n       title = \"Public Expenditures vs Percent Labor Union\")\n\n  \nprint(nations_plot)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "chapter3.html#pages-44-to-46",
    "href": "chapter3.html#pages-44-to-46",
    "title": "3  Chapter 3",
    "section": "3.2 Pages 44 to 46",
    "text": "3.2 Pages 44 to 46\n\n3.2.1 Opioid vs Satisfaction Analysis\n\n3.2.1.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n3.2.1.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/StateData2018.csv\",\n                save_as = \"statedata2018.csv\")\n\n\n\n3.2.1.3 Load data from your computer\nClean names is essential here. It makes sure all column names are lower case. They are not all lower case in the original data file.\n\nstatedata2018 <- read_csv(\"statedata2018.csv\") %>% \n  clean_names()\n\n\n\n3.2.1.4 Inspect data\nThere are so many column names in this data set that we do the glimpse a bit differently. That is, we sort the order of the columns alphabetically prior to doing the glimpse(). It affects only the display of the column names - not the structure of the data.\n\nstatedata2018 %>% \n  select(sort(names(statedata2018))) %>%\n  glimpse()  \n\nRows: 50\nColumns: 77\n$ aa_voted_percent           <dbl> 49.6, NA, 50.9, 41.6, 49.9, 31.0, 48.3, 55.…\n$ alc_disorder_past_year     <dbl> 4.51, 6.57, 5.49, 5.38, 5.51, 6.30, 6.10, 6…\n$ assault_rate               <dbl> 283.4, 440.2, 252.1, 346.0, 236.6, 192.8, 1…\n$ assoc_degree               <dbl> 8.2, 8.5, 8.4, 6.7, 7.7, 8.6, 7.5, 7.9, 9.7…\n$ bach_degree                <dbl> 15.4, 19.0, 18.1, 14.2, 20.6, 24.9, 21.9, 1…\n$ binge_alc_past_month       <dbl> 21.40, 24.20, 23.10, 19.49, 23.52, 26.97, 2…\n$ burglary_rate              <dbl> 819.0, 427.6, 647.1, 835.7, 522.3, 438.2, 3…\n$ census_division            <chr> \"EastSouthCentral\", \"Pacific\", \"Mountain\", …\n$ census_region              <chr> \"South\", \"West\", \"West\", \"South\", \"West\", \"…\n$ comm_supervision_rate      <dbl> 1591.14, 1522.95, 1503.95, 2222.08, 1088.59…\n$ community_supervision      <dbl> 60700, 8400, 84800, 51500, 333300, 90900, 4…\n$ conservative               <dbl> 26.00835, 19.27924, 20.40239, 25.04501, 17.…\n$ cost_living                <dbl> 89.3, 129.9, 97.0, 86.9, 151.7, 105.6, 127.…\n$ death_row                  <dbl> 177, 0, 120, 32, 727, 3, 0, 0, 348, 49, 0, …\n$ death_row_rate             <dbl> 4.33, 0.00, 2.18, 1.33, 3.59, 0.09, 0.00, 0…\n$ doctoral_degree            <dbl> 1.0, 1.2, 1.2, 0.9, 1.6, 1.6, 1.6, 1.8, 1.1…\n$ dom_mig_rate               <dbl> 1.17, -14.58, 11.61, 0.82, -3.95, 7.60, -6.…\n$ dom_migration              <dbl> 5718, -10752, 83240, 2475, -156068, 43293, …\n$ fips_code                  <dbl> 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 15, 16, 17…\n$ gross_state_product        <dbl> 211197, 51479, 326446, 122704, 2797601, 345…\n$ guns                       <dbl> 161641, 15824, 179738, 79841, 344622, 92435…\n$ guns_per_capita            <dbl> 33.15, 21.38, 25.61, 26.57, 8.71, 16.48, 22…\n$ health_exp_per_capita      <dbl> 7281, 11064, 6452, 7408, 7549, 6804, 9859, …\n$ illicit_disorder_past_year <dbl> 2.86, 3.69, 2.79, 2.80, 2.97, 3.43, 3.47, 3…\n$ illicit_drugs_past_month   <dbl> 8.67, 16.81, 10.77, 10.51, 13.11, 17.83, 12…\n$ infant_mortality_rate      <dbl> 9.03, 5.17, 5.32, 8.20, 4.22, 4.76, 4.78, 7…\n$ intern_mig_rate            <dbl> 0.68, 3.26, 2.00, 0.75, 2.98, 1.44, 4.62, 1…\n$ intern_migration           <dbl> 3344, 2401, 14335, 2260, 117797, 8207, 1649…\n$ larceny_rate               <dbl> 2149.5, 2096.4, 2289.1, 2313.5, 1527.4, 185…\n$ latinx_voted_percent       <dbl> 29.1, NA, 48.8, 20.8, 43.3, 43.9, 41.1, NA,…\n$ life_expectancy            <dbl> 75.0, 77.8, 79.2, 75.5, 81.0, 80.0, 80.7, 7…\n$ life_satis                 <dbl> 51.42162, 52.08266, 51.38469, 52.34836, 51.…\n$ masters_degree             <dbl> 6.9, 7.6, 7.8, 5.9, 8.4, 10.9, 12.1, 8.9, 7…\n$ med_hh_income              <dbl> 48486, 76715, 56213, 45726, 71228, 68811, 7…\n$ median_age                 <dbl> 39.0, 33.5, 37.5, 38.0, 36.4, 36.7, 40.9, 4…\n$ mental_illness_past_year   <dbl> 19.95, 20.32, 18.02, 20.73, 18.18, 19.86, 1…\n$ motor_vehicle_theft_rate   <dbl> 209.1, 236.0, 261.3, 188.8, 391.3, 234.8, 1…\n$ murder_ms_rate             <dbl> 5.7, 5.6, 4.7, 5.6, 4.4, 2.8, 2.4, 5.8, 5.8…\n$ opioid_od_death_rate       <dbl> 9.0, 13.9, 13.5, 6.5, 5.3, 10.0, 27.7, 27.8…\n$ pain_pill_past_year        <dbl> 4.53, 4.81, 4.27, 5.03, 4.30, 4.87, 4.21, 4…\n$ per_age0_18                <dbl> 24, 27, 24, 25, 24, 24, 22, 22, 21, 26, 23,…\n$ per_age19_25               <dbl> 9, 9, 9, 9, 9, 9, 8, 8, 8, 9, 7, 9, 9, 9, 9…\n$ per_age26_34               <dbl> 12, 13, 12, 12, 14, 14, 11, 12, 12, 12, 12,…\n$ per_age35_54               <dbl> 25, 26, 24, 25, 26, 26, 27, 25, 25, 27, 25,…\n$ per_age55_64               <dbl> 14, 13, 12, 13, 12, 13, 15, 14, 14, 12, 13,…\n$ per_age65plus              <dbl> 17, 12, 18, 17, 14, 14, 17, 19, 21, 14, 19,…\n$ per_am_ind_alaskan         <dbl> 1.0, 16.0, 4.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.…\n$ per_asian                  <dbl> 1, 6, 3, 2, 15, 3, 5, 4, 3, 4, 38, 2, 6, 2,…\n$ per_black                  <dbl> 26.0, 3.0, 4.0, 15.0, 5.0, 4.0, 10.0, 21.0,…\n$ per_cap_income             <dbl> 26846, 35874, 29265, 25635, 35021, 36415, 4…\n$ per_child_poverty          <dbl> 24, 13, 20, 24, 17, 12, 14, 17, 19, 20, 12,…\n$ per_latinx                 <dbl> 4, 7, 32, 8, 39, 22, 17, 9, 26, 10, 10, 13,…\n$ per_pop_change2010_18      <dbl> 2.254, 3.828, 12.192, 3.354, 6.181, 13.247,…\n$ per_poverty                <dbl> 16.89, 11.11, 14.86, 16.40, 13.30, 10.29, 9…\n$ per_white                  <dbl> 66, 60, 54, 72, 37, 68, 66, 62, 53, 52, 21,…\n$ percent_uninsured          <dbl> 12.0, 14.3, 12.7, 9.8, 8.3, 8.6, 6.2, 6.8, …\n$ pop_change2010_18          <dbl> 107733, 27189, 779358, 97797, 2302522, 6662…\n$ pop_density                <dbl> 95.4, 1.3, 58.3, 56.9, 246.1, 50.8, 742.6, …\n$ pop18and_older             <dbl> 3814879, 551562, 5638481, 2317649, 30617582…\n$ population                 <dbl> 4887871, 737438, 7171646, 3013825, 39557045…\n$ prison_rate                <dbl> 1072.12, 797.73, 975.44, 1035.53, 662.04, 7…\n$ prisoners                  <dbl> 40900, 4400, 55000, 24000, 202700, 32100, 1…\n$ professional_degree        <dbl> 1.5, 1.8, 1.8, 1.3, 2.4, 2.3, 3.0, 1.9, 2.1…\n$ prop_crime_rate            <dbl> 3177.6, 2760.0, 3197.5, 3338.0, 2441.1, 253…\n$ rape_rate                  <dbl> 41.3, 104.7, 50.2, 59.4, 29.7, 56.7, 21.7, …\n$ religious                  <dbl> 32.03850, 24.12306, 26.40519, 30.05092, 24.…\n$ robbery_rate               <dbl> 96.9, 85.4, 92.8, 69.1, 125.5, 56.7, 87.8, …\n$ state                      <chr> \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\",…\n$ state_taxes_per_capita     <dbl> 2262, 2226, 2272, 3266, 4424, 2599, 5341, 4…\n$ suic_rate_age_adj          <dbl> 16.6, 27.0, 18.2, 20.8, 10.5, 20.3, 10.5, 1…\n$ suicide_idea_past_year     <dbl> 3.89, 5.34, 4.02, 4.59, 4.04, 5.41, 4.05, 4…\n$ suicide_rate               <dbl> 15.90, 28.07, 19.37, 21.72, 11.48, 21.59, 1…\n$ tobacco_past_month         <dbl> 30.12, 26.68, 21.19, 30.69, 16.23, 20.15, 2…\n$ total_voted_percent        <dbl> 69.0, 67.7, 68.6, 58.5, 61.5, 65.6, 68.0, 6…\n$ unemploy_rate              <dbl> 4.1, 6.5, 4.7, 3.5, 4.1, 3.1, 4.2, 4.0, 3.5…\n$ violent_crime_rate         <dbl> 427.4, 635.8, 399.9, 480.1, 396.1, 309.1, 2…\n$ white_voted_percent        <dbl> 52.6, 60.3, 65.3, 44.0, 61.4, 62.6, 57.7, 5…\n\n\n\n\n3.2.1.5 Graph\n\nstate_plot <- ggplot(data = statedata2018,\n                     mapping = aes(x = life_satis,\n                                   y = opioid_od_death_rate)) +\n  geom_point(shape = 1) +\n  geom_text(mapping = aes(label = state),\n            nudge_y = 1,\n            size = 2) +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"red\") +\n  coord_cartesian(xlim = c(49, 55),\n                  ylim = c(0, 50)) +\n  scale_x_continuous(breaks = seq(49, 55, by = 1)) +\n  scale_y_continuous(breaks = seq(10, 50, by = 10)) +\n  labs(x = \"Average Life Satisfaction\",\n       y = \"Opioid overdose deaths per 100,000\",\n       title = \"Opioid Deaths vs Life Satisfaction\") +\n  theme_light()\n\nprint(state_plot)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n3.2.1.6 Analysis using summary()\n\nlrm3_1 <- lm(opioid_od_death_rate ~ life_satis,\n             data = statedata2018)\n\nsummary(lrm3_1)\n\n\nCall:\nlm(formula = opioid_od_death_rate ~ life_satis, data = statedata2018)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.688  -6.952  -1.511   3.408  28.118 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  212.056     56.576   3.748 0.000479 ***\nlife_satis    -3.792      1.093  -3.468 0.001116 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.292 on 48 degrees of freedom\nMultiple R-squared:  0.2004,    Adjusted R-squared:  0.1837 \nF-statistic: 12.03 on 1 and 48 DF,  p-value: 0.001116\n\nconfint(lrm3_1)\n\n                2.5 %     97.5 %\n(Intercept) 98.302367 325.809877\nlife_satis  -5.989863  -1.593489\n\n\n\n\n3.2.1.7 Analysis apa.reg.table()\nUsing apaTables to display the regression results is probably a better approach. It’s one step and combines everything into one table. But summary() approach does give additional information (e.g., adjusted \\(R^2)\\)) - so it’s good to be familiar with both approaches.\n\nlrm3_1 <- lm(opioid_od_death_rate ~ life_satis,\n             data = statedata2018)\n\nlibrary(apaTables)\napa.reg.table(lrm3_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\n\n\n\n\n(Intercept)\n212.06**\n[98.30, 325.81]\n\n\n\n\n\n\n\n\nlife_satis\n-3.79**\n[-5.99, -1.59]\n-0.45\n[-0.71, -0.19]\n.20**\n[.04, .38]\n-.45**\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .200**\n\n\n\n\n\n\n\n\n\n\n95% CI[.04,.38]"
  },
  {
    "objectID": "chapter3.html#pages-49-to-50",
    "href": "chapter3.html#pages-49-to-50",
    "title": "3  Chapter 3",
    "section": "3.3 Pages 49 to 50",
    "text": "3.3 Pages 49 to 50\n\n3.3.1 Fitted values\nWhen you look at the graph with life_satis on the x-axis and opioid_od_death_rate on the y-axis you see a regression line. The points that fall on this line are predicted scores for opioid_od_death_rate based on life_satis. Alternatively, we might call these fitted values for opioid_od_death_rate based on life_satis. We can obtained a predicted score (i.e., fitted score) for each x-axis value using augment() command fromm the broom package. When you inspect the output below only pay attention to the opioid_od_death_rate, life_satis, and .fitted columns. The column pioid_od_death_rate is the measured opiod overdose death rate, the column life_satis is the measured life satisfaction, the column .fitted is the predicted opiod overdose death rate for a given life_satis value.\n\nlrm3_1 <- lm(opioid_od_death_rate ~ life_satis,\n             data = statedata2018)\n\nlibrary(broom)\nlrm3_1 %>%\n  augment()\n\n# A tibble: 50 × 8\n   opioid_od_death_rate life_satis .fitted  .resid   .hat .sigma   .cooksd\n                  <dbl>      <dbl>   <dbl>   <dbl>  <dbl>  <dbl>     <dbl>\n 1                  9         51.4    17.1  -8.08  0.0214   9.31 0.00844  \n 2                 13.9       52.1    14.6  -0.676 0.0217   9.39 0.0000598\n 3                 13.5       51.4    17.2  -3.72  0.0217   9.37 0.00182  \n 4                  6.5       52.3    13.6  -7.07  0.0252   9.33 0.00768  \n 5                  5.3       51.5    16.8 -11.5   0.0208   9.24 0.0167   \n 6                 10         52.9    11.6  -1.56  0.0381   9.39 0.000579 \n 7                 27.7       50.5    20.6   7.08  0.0415   9.33 0.0131   \n 8                 27.8       52.7    12.1  15.7   0.0340   9.10 0.0521   \n 9                 16.3       51.1    18.4  -2.13  0.0262   9.38 0.000726 \n10                  9.7       51.2    17.9  -8.23  0.0240   9.31 0.00988  \n# ℹ 40 more rows\n# ℹ 1 more variable: .std.resid <dbl>\n\n\n\n\n3.3.2 Fitted values and percentiles\nSee previous section for loading the data.\nWe want to predict opioid overdose at the 25th, 50th, and 75th percentiles for life satisfaction. So we obtain the life satisfaction values corresponding to these percentiles below.\n\nstatedata2018 %>%\n  select(life_satis) %>%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n50\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlife_satis\n0\n1\n51.73\n1.21\n49.07\n51.08\n51.63\n52.33\n55.63\n▃▇▇▂▁\n\n\n\n\n\nFrom the above skim() output we extract the percentile information and put it in a table that’s easy to follow below. We can see the life_satis value for each percentile in this table.\n\n\n\nPercentile\nlife_satis value\n\n\n\n\n25th\n51.1\n\n\n50th\n51.6\n\n\n75th\n52.3\n\n\n\n\n\n3.3.3 Calculate fitted values\n\nlrm3_1 <- lm(opioid_od_death_rate ~ life_satis,\n             data = statedata2018)\n\n\n# we need to use the EXACT name from the original data set\nlife_satis <- c(51.1, 51.6, 52.3)\n\nfit_for_values = data.frame(life_satis)\n\npredict(lrm3_1, fit_for_values)\n\n       1        2        3 \n18.30148 16.40564 13.75146 \n\n\nThe values above are the predicted values for opioid_od_death_rate. We put everything in the table below for clarity. The predicted opioid_od_death_rate value provides a corresponding point on the regression line. That is, all (life_satis value, predicted opioid_od_death_rate value) points fall on the regression line.\n\n\n\n\n\n\n\n\nPercentile life_satis\nlife_satis value\npredicted opioid_od_death_rate value\n\n\n\n\n25th\n51.1\n18.30148\n\n\n50th\n51.6\n16.40564\n\n\n75th\n52.3\n13.75146\n\n\n\nRecall the regression formulas:\n\\[\n\\hat{y} = b_0 + b_1X\n\\]\nIn the context of our variables: \\[\n\\widehat{opioid\\_od\\_death\\_rate} = b_0 + b_1(life\\_satis)\n\\]\nRecall the regression output:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\n\n\n\n\n(Intercept)\n212.06**\n[98.30, 325.81]\n\n\n\n\n\n\n\n\nlife_satis\n-3.79**\n[-5.99, -1.59]\n-0.45\n[-0.71, -0.19]\n.20**\n[.04, .38]\n-.45**\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .200**\n\n\n\n\n\n\n\n\n\n\n95% CI[.04,.38]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom this output we get the regression formula:\nThis formula \\[\n\\widehat{opioid\\_od\\_death\\_rate} = b_0 + b_1(life\\_satis)\n\\]\nBecomes: \\[\n\\widehat{opioid\\_od\\_death\\_rate} = 212.06 + -3.79(life\\_satis)\n\\]\nTherfore for our three points:\n\\[\n\\begin{aligned}\n18.30148 &= 212.06 + -3.79(51.1)\\\\\n16.40564 &= 212.06 + -3.79(51.6)\\\\\n13.75146 &= 212.06 + -3.79(52.3)\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapter3.html#chapter-exercises",
    "href": "chapter3.html#chapter-exercises",
    "title": "3  Chapter 3",
    "section": "3.4 Chapter Exercises",
    "text": "3.4 Chapter Exercises\n\n3.4.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n3.4.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/HighSchool.csv\",\n                save_as = \"highschool.csv\")\n\n\n\n3.4.3 Load data from your computer\n\nhighschool <- read_csv(\"highschool.csv\") %>% \n  clean_names()\n\n\n\n3.4.4 Inspect data\n\nhighschool %>% \n  glimpse()  \n\nRows: 178\nColumns: 6\n$ row                  <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ id_number            <dbl> 2583454, 758606, 6918338, 757890, 4584594, 858964…\n$ sports_participation <dbl> 0.00, 1.10, 0.00, 0.00, 1.10, 1.10, 0.69, 0.00, 1…\n$ academic_clubs       <dbl> 1.61, 0.69, 1.10, 0.00, 0.69, 0.00, 0.69, 1.61, 1…\n$ alcohol_use          <dbl> 1.39, 0.69, 0.69, 0.69, 0.00, 1.10, 1.95, 0.69, 1…\n$ gpa                  <dbl> 3.25, 4.00, 4.00, 2.88, 4.00, 2.25, 2.12, 3.62, 3…"
  }
]